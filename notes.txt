18-04-2024

DEVOPS: TO DELIVER THE APP/PsRODUCT/SOFTWARE SPEEDILY

SDLC: SOFTWARE DEVELOPMENT LIFECYCLE
DEPLOYMENT: process of installing the 6application to the server.

HUMANS: Infant -- > Toddler -- > Child -- > Teen -- > Adult -- > Old
BABY: MEN & WOMENT

APPLICATION:
DEV & OPS  

PLAN
CODE 
BUILD
TEST
DEPLOY
OPERATE
MONITOR



COURSE: DEVOPS WITH AWS
FREE: LINUX
DURATION: 2.5 MONTHS 
FEE: 10K (16K FOR REC +)
TIMINGS: 6:00 TO 7:30 PM
CLASS: 70 MINUTES (2O MINS INTERACTIVE)
EXP: 3+ 
LABS: 9 AM - 7 PM
PROJECTS: 5 PROJECTS
TOOLS: 18 TOOLS

=============================================================================

SOFTWARE ARCHITECTURE:
Blueprint of the application workflow.

TYPES:
1. ONE-TIER ARCHITECTURE
2. TWO-TIER ARCHITECTURE
3. THREE-TIER ARCHITECTURE
4. N-TIER ARCHITECTURE

TIER = LAYER = SERVER

SERVER: to serve the services to end user.

1. web server  : presentation layer
2. app server  : business/logic layer
3. db server   : data server


WEB SERVER: httpd
AKA	  : the presentation layer
Purpose   : to show the app
Who	  : UI/UX (front-end dev)
What	  : Web Technologies
EX	  : html, css, js


APP SERVER: tomcat
AKA	  : Business/Logic layer
Purpose	  : to use the app
Who       : Backend Dev
What	  : Programming languages
Ex        : java, python, c, c++, .net


DB SERVER: mysql
AKA       : data server
Purpose   : to store & retrieve data
Who	  : DBA/DBD
What      : DB languages
Ex	  : mysql, mssql, postgres, arango, oracle ---



1. ONE-TIER ARCHITECTURE : (STANDALONE APP)
ALL 3 LAYERS WILL BE ON LOCAL.
HERE THE APP WILL WORK ON OUR LAPTOP.
IT DOESN'T REQ INTERNET CONNECTION.

EX: VLC 

2. TWO-TIER ARCHITECTURE : (CLIENT-SERVER APP)
WEB & APP = LOCAL 
DB = INTERNET
IT REQ INTERNET CONNECTION.

EX: BANKING APPS

3. THREE-TIER ARCHITECTURE : (WEB APP)
APP NEED NOT TO INSTALL
WE CAN USE ANY APP FROM THE INTERNET

EX: ALL THE APP
=====================================================================================
SERVER: Serves the services to end user.
it will take request from user and send response.


PHYSICAL SERVERS:
 
1. It will consume space
2. 24/7 electricity
3. maintenance

CLOUD SERVERS:
AWS
AZURE
GCP

AWS : AMAZON WEB SERVICES
SERVER: EC2 -- > ELASTIC COMPUTE CLOUD


TO CREATE SERVER ON AWS, IT WILL TAKE 7 STEPS

1. NAME & TAGS : 

2. AMI : AMAZON MACHINE IMAGE
   WE HAVE OS, SOFTWARE PKGS

3. INSTANCE TYPE: CPU & MEMORY
   T2.MICRO : 1 CPU & 1 RAM 

4. KEYPAIR: USED TO LOGIN 
   HERE WE HAVE 2 KEYS 
   1. PUBLIC KEY   : AWS
   2. PRIVATE KEY  : USER   (PEM, PPK)

5. NETWORK: 
   VPC : PRIVACY & SECURITY FOR SERVER
   SG  : TO ACCESS THE APPLICATIONS (PORTS : 0 - 65535)

6. STORAGE: EBS VOLUME
   TO PROVIDE STORAGE FOR THE SERVER 
   DEFAULT: 8GB   MAX: 16 TB

7. summary



LINUX:
It is an os -- > Blender

os: Operating system: 
its a software used to communicate with systems.

Linux is a Kernel.
Kernel: it is lowest level of operating system.


WHY WE NEED TO LEARN LINUX:
1. TOP 500 SUPER COMPUTERS
2. 96% SERVERS
3. 86% mobiles
4. Electronic Gadgets

ADVANTAGES:
1. FREE AND OPEN-SOURCE
2. SECURITY WILL BE HIGH
3. HIGH PERFORMANCE
4. MULTI USER BASED
5. IT SUPPORTS ALL PROGRAMMING LANGUAGES
6. HIGH SPEED

COMPONENTS:
1. SHELL: deal with input & outputs.
2. DAEMON: deal with background processs.
3. Kernel: deals with hardware. (cpu, ram, rom)

FLAVOURS/DISTRIBUTION:
IPHONE15 -- > 15 + -- > 15 PRO -- > PRO MAX

UBUNTU   : 70%
REDHAT
CENTOS
FEDORA
DEBIAN
ROCKY LINUX
KALI LINUX
ALMA LINUX


TYPES:
1. GUI : GRAPHICAL USER INTERFACE
2. CLI : COMMAND LINE INTERFACE

COMMANDS:

sudo -i		: to login as root user
uname 		: to show the os name
uname -r	: to show the os kernel info
uname -a	: to show complete info of os
who/w		: to show the logged user info
whoami		: to show current user info
clear/ ctrl l	: to clear the screen
logout /ctrl d	: to logout from the current user
date		: time and date
hostname 	: to print the hostname
hostnamectl set-hostname raham : it will set the hostname

FILE COMMANDS:

touch filename: to create a file
ls/ll			: to list the file
cat file1		: to show content in file1
more file1		: to show content in file1
cat>>file1		: to insert the content in a file
enter + ctrl d 		: to save the content
cp file1 file2		: to copy content from file1 to file2
mv file1 raham		: to rename file1 as raham
rm file1		: to remove a file
rm file3 -f		: to remove a file forcefully
rm * -f			: to remove all files forcefully
touch file{1..100}	: to create file1 to file100
wc file1		: to print lines, words and characters of a file


head file1		: to print top 10 lines
head -5 file1		: to print top 5 lines
head -12 file1		: to print top 12 lines
tail file1		: to print bottom 10 lines
tail -5 file1		: to print bottom 5 lines
tail -12 file1		: to print bottom 12 lines
sed -n '7,14p' file1	: to print from line 7 to 14

mkdir dir1		: to create a directory
cd dir1			: to change to dir1
cd ..			: to come a dir back 

HARDWARE COMMANDS:
cat /proc/cpuinfo	: to show cpu information
lscpu			: to show cpu information
cat /proc/meminfo	: to show memory information
lsmem			: to show memory information
fdisk -l		: to show ebs volume
lsblk			: to show ebs volume
yum install lshw	: to install lshw package
lshw			: to show complete hardware information
cat /etc/os-release	: to show flavour of os


==========================================================================================

EDITORS:
is to edit/modify the content in a file.
types:
1. vi/vim
2. nano

i	: to insert the conetnt
esc	: to exit from insert mode

1. command mode
2. insert mode
3. save mode


SAVE MODE:
:w	: to save the content
:q	: to quit from file
:wq	: to save & quit 
:wq!	: to save & quit forcefully

INSERT MODE:
i	: to insert the content
I	: starting of the line
A	: to end of the line
O	: creats new line above existing line
o	: crerate new line below existing line

COMMAND MODE:

gg	: top of file
shift g	: bottom of file
:set nu : print number of lines inside the file
:10	: to go to 10th line (10gg)
yy	: to copy single line
3yy	: to copy three lines
p	: to past for single time
3p	: to past for 3 times
dd	: to delete single line
3dd	: to delete three lines
u	: undo
ctrl r	: to redo

==================================================================

GREP: Global Regular Expression Print
Purpose: to find the words in a file


grep word file1		: to search for a word in file1
grep word file1 -c	: to print the count of word
grep word file1 -i	: ignores case sensitive
grep word file1	-v	: to avoid the lines which have the word
grep 'word1\|word2' file1: to search for multiple words



==============================================================

GIT : Global Information Tracker

SCM : Source Code Management 
VCS : Version Control System

WHY GIT: to store each version of code seperately.
Purpose: To RollBack

Rollback means Going back to previous version of the application.
V2 < -- v3

GIT:

Git is used to track the files.
It will maintain multiple versions of the same file.
It is platform-independent.
It is free and open-source.
They can handle larger projects efficiently.
It is 3rd generation of vcs.
it is written on c programming
it came on the year 2005


V1 : INDEX.HTML = 10 lines
V2 : INDEX.HTML = 20 lines
V3 : INDEX.HTML = 30 lines


CVCS : Centralized Version Control System
stores code on single repo.
ex: svn

DVCS : Distributed Version Control System
stores code on multiple repos.
ex: git

STAGES OF GIT:

1. Working Directory   : where we write the code
2. Staging Area        : where we track the code
3. Repository          : where we store the tracked code.


PRACTICAL PART:
1. CREATE A SERVER AND LOGIN TO IT

sudo -i
mkdir paytm
cd paytm

yum install git -y
git init    (whithout .git we cant run git commands)

touch index.html	: to create a file
git status		: to check status of a file
git add index.html	: to track the file
git commit -m "commit-1" index.html : to commit the file

git log			: to show commits history
git log	--oneline	: to show commits history in single line
git log	--oneline -2	: to show last 2 commits history
git show commit_id	: to show which file is commited for this id.


USER CONFIGURATION:
by default all the commits will come with root user
so we need to chnage the username as per our requirment

git config user.name "raham"
git config user.email "raham@gmail.com"


GIT RESTORE:
this command will untrack the tracked file.
git restore --staged file_name
git rm --cached raham (alternative command)

this command used to recover the deleted file.
git restore file_name

GIT STASH:
to hide the tracked files for temperory purpose.

touch file1
git add file

git stash	: to stash the file
git stash apply	: to unstash the file
git stash list	: to list the stashes
git stash clear	: to delete the stashes


DAY: 29-04-2024 :


BRANCHES:
It's an individual line of development for code.
we create different branches in real-time.
each developer will work on their own branch.
At the end we will combine all branches together.
Default branch is Master.

git branch		: to list the branches
git branch movies	: to create a new branch
git checkout movies	: to switch from one branch to another.
git checkout -b recharge: to create and switch from one branch to another.
git branch -m old new	: to rename a branch
git branch -D movies	: to delete a branch

we cant delete the current branch

PROCESS:

git branch		
git branch movies	
git checkout movies
touch movies{1..5}
git status
git add movies*
git commit -m "dev-1 commits" movies*
git branch dth	
git checkout dth
touch dth{1..5}
git status
git add dth*
git commit -m "dev-2 commits" dth*
git checkout -b train
touch train{1..5}
git add train*
git commit -m "dev-3 commits" train*
git checkout -b recharge
touch recharge{1..5}
git add recharge*
git commit -m "dev-4 commits" recharge*


Note: here every dev works on the local laptop
at the end we want all dev codes to create an application.
so here we use GitHub to combine all dev codes together.

Create a GitHub account and create Repo 


git remote add origin https://github.com/revathisiriki78/paytm.git
git push origin movies
username:
password:

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
Note: in github passwords are not accepted we need to use token 

profile -- > settings -- > developer settings -- > Personal access token -- > classic -- > 
generate new token -- > classic -- > name: paytm -- > select 6 scopes -- > generate 

git push origin dth
username:
password:


git push origin train
username:
password:

git push origin recharge
username:
password:

=================================================================

GIT CLONE: it download code from github(Remote) to git(Local).
git clone https://github.com/anitalluri00/paytm.git


GIT FORK: it download code from github account to another.
For git clone and git fork repos must be public.

PUBLIC REPO: source code will be visible from internet.
PRIVATE REPO: source code will be hidden from internet.


GIT MERGE: it will merge files blw two different branches

git checkout master
git merge movies
git merge train


GIT REBASE: used to add files blw two different branches
git checkout master
git rebase train
git rebase recharge


MERGE VS REBASE:
merge for public repos, rebase for private 
merge stores history, rebase will not store the entire history
merge will show files, rebase will not show files


in GitHub we use Pull Request (PR) to do merging.


GIT REVERT: used to revert(get back) the files we merge.
to undo merge we can use revert.

git revert dth
git revert recharge


======================================================

GIT PULL:
used to get the changed files from github to git.
git pull origin master

GIT FETCH:
used to show the changed files from github to git.
git fetch

CHERRY-PICK: Merging the specific files based on commits.
git cherry-pick commit_id

Process:

mkdir raham
cd raham/
touch java{1..3}
git init
git add java*
git commit -m "java-commits" java*
git branch
git checkout -b branch1
touch python{1..5}
git add python*
git commit -m "python commits" python*
touch php{1..5}
git add php*
git commit -m "php commits" php*
touch donet{1..5}
git add donet*
git commit -m "dontent commits" donet*
git log --oneline
git checkout master
ll
git cherry-pick commit_id_of python
ll
git cherry-pick commit_id_of php


MERGE CONFLICTS:
it will rise when we merge 2 different branches with same files.
How to resolve: Manually 

vim index.html
im dev-1 writing java on branch-1
git add index.html
git commit -m "dev-1 commits" index.html
git branch -m master branch1

git checkout -b branch2
vim index.html
im dev-2 writing index.html on branch-2
git add index.html
git commit -m "dev-2 commits" index.html

vim index.html
im dev-1 writing index.html on branch-1
new line
git add index.html
git commit -m "dev-1 2nd commits" index.html
git merge branch2


vim index.html
git add index.html
git commit -m "merge commits"


.gitignore: this file will ignore the files of being tracked.
if you write any filename on this .gitingore it wont track that file.

Note: . should be mandatory


GIT RESET: to undo the commits.

LONG LIVING BRANCHES: these branches we wont delete and we use them frequently.
EX: Master, Main

SHORT LIVING BRANCHES: these branches we will delete and we dont use them frequently.
EX: release, bugfix, --------

========================================================
02-05-2024


MAVEN:

raw chicken -- >  clean  --> marnet -- > ingredients -- > Chicken Biryani
raw code    -- > build   -- > test  -- > artifact -- > Deployment

ARTIFACT: its final product of our code.
developers will give raw code that code we are going to convert into artifact.

TYPES:
1. jar	: Java Archive       : Backend code
2. war	: Web Archive	     : Frontend code + Backend code
3. ear	: Enterprise Archive : jar + war 

JAR FILE:

.java -- > compile -- > .class -- > .jar file

.java	: basic raw
.class	: executable file
.jar	: artifact

all the artifacts are going to created by a build too1.

MAVEN:
Maven is the build tool.
its a free and opensource.
build: process of adding the libs & dependencies to code.
its is also called as Project management too1.
it will manage the complete structure of the project.
the main file in the maven tool is POM.XML

POM.XML: its a file which consist of complete project information.
Ex: name, artifact, tools, libs, dep --------

POM: PROJECT OBJECT MODEL
XML: EXTENSIBLE MARKUP LANGUAGE

WHO GIVE POM.XML : DEVELOPERS
dev will give both code and pom.xml in github

maven is written on java by apache software foundation.
supports: JAVA-1.8.0
year: 2004
home path: .m2


PRATCICAL PART:
1. CREATE AN EC2 INSTANCE AND CONNECT
2. yum install git java-1.8.0-openjdk maven tree -y
3. git clone https://github.com/devopsbyraham/jenkins-java-project.git
4. cd jenkins-java-project


MAVEN LIFECYCLE:
GOALS : a command used to run a task.
Goals refers pom.xml to execute.


PLUGIN: its a small software with makes our work automated.
insted of downloading tools we can donwload plugins.
this plugins will donwload automatically when we run goals.



1. mvn compile : used to compile the code (.java [src] -- > .class [target])
2. mvn test    : used to test the code    (.java [test] -- > .class [target])
3. mvn package : used to create artifact 
4. mvn install : used to copy artifact to .m2 (project folder -- > .m2)
5. mvn clean   : to delete the target folder
6. mvn clean package: compile -- > install


PROBLEMS WITHOUT MAVEN:
1. we cant create artifacts.
2. We cant create project structure.
3. we cant build and deploy the apps.

ALTERNATIVIES:
MAVEN, ANT, GRADLE 

MAVEN VS ANT:
1. MAVEN IS BUILD & PROJECT MANAGEMNT, ANT IS ONLY BUILD TOOL
2. MAVEN HAS POM.XML, ANT HAS BUILD.XML
3. MAVEN HAS A LIFECYCLE, ANT WILL NOT HAVE LIFECYCLE
4. MAVEN PLUGINS ARE REUSABLE, ANT SCRIPTS ARE NOT RESUEABLE.
5. MAVEN IS DECLARATIVE, ANT IS PROCEDURAL.

PROGRAMMING VS BUILD:

JAVA	: MAVEN
PYTHON	: GRADLE
.NET	: VS CODE
C, C#	: MAKE FILE
node.js	: npm

ALTERNETIAVES: 
ANT, GRADLE for java projects.

HISTORY:
    1  yum install git java-1.8.0-openjdk maven tree -y
    2  mvn -v
    3  git clone https://github.com/devopsbyraham/jenkins-java-project.git
    4  ll
    5  cd jenkins-java-project/
    6  ll
    7  tree
    8  cat pom.xml
    9  tree
   10  ll
   11  mvn compile
   12  ll
   13  tree
   14  mvn test
   15  tree
   16  mvn package
   17  tree
   18  mvn install
   19  ll /root/.m2/repository/in/RAHAM/NETFLIX/1.2.2/
   20  ll
   21  mvn clean
   22  ll
   23  mvn install
   24  ll
   25  mvn clean
   26  mvn clean packge
   27  mvn clean package
   28  ll
   29  mvn clean
   30  ll
   31  mvn compile
   32  mvn test
   33  mvn package
   34  tree
   35  mvn install
   36  mvn clean
   37  mvn clean package
   38  ll target/
   39  tree
   40  ll
   41  cd
   42  mvn clean
   43  'll
   44  ll
   45  cd jenkins-java-project/
   46  ll
   47  mvn clean
   48  cd
   49  ls -al
   50  history

===============================================================================


JENINS IS A CI/CD TOOL.
REALITY: JENKINS IS ONLY FOR CI.

CI : CONTINOUS INTEGRATION : CONTINOUS BUILD + CONTINOUS TEST (OLD CODE WITH NEW CODE)

DAY-1: 100 LINES : BUILD + TEST
DAY-2: 200 LINES : BUILD + TEST
DAY-3: 300 LINES : BUILD + TEST

BEFORE CI:
MANUAL PROCESS
TIME WASTE

AFTER CI:
AUTOMATED PROCESS
TIME SAVING

CD: CONTINOUS DELIVERY/DEPLOYMENT

ENV:

PRE-PROD/NON-PROD:
DEV	: developers
QA	: testers
UAT	: clients

LIVE/PROD ENV:
PROD	: users


CONTINOUS DELIVERY: Deploying the application to producion in manual.
CONTINOUS DEPLOYMENT: Deploying the application to producion in automatic.


PIPELINE: 

WAKEUP -- > DAILY ACTIVITIES -- > BREAKFAST -- > LUNCH -- > CLASS
CODE -- > COMPILE -- > TEST -- > ARTIFACT -- > DEPLOY

SETP BY STEP EXECUTION OF A PROCESS.
SERIES OF EVENTS INTERLINKED WITH EACHOTHER.


JENKINS: 
ITS A FREE AND OPEN-SOURCE TOOL.
JENKINS WRITTEN ON JAVA.
IT IS PLATFORM INDEPENDENT.
IT CONSIST OF PLUGINS.
WE HAVE COMMUNITY SUPPORT.
IT CAN AUTOMATE ENTIRE SDLC.
IT IS OWNED BY SUN MICRO SYSTEM AS HUDSON.
HUDSON IS PAID VERSION.
LATER ORACLE BROUGHT HUDSON AND MAKE IT FREE.
LATER HUDSON WAS RENAMED AS JENINS.
INVENTOR: Kohsuke Kawaguchi
PORT NUMBER: 8080
JAVA: JAVA-11/17
DEFAULT PATH: /var/lib/jenkins

ALTERNATIVES:
BAMBOO, GO CI, CIRCLE CI, TARVIS, SEMAPHORE, BUDDY BUILD MASTER, GITLAB, HARNESS
ARGOCD -----

CLOUD: AWS CODEPIPELINE, AZURE PIPLEINE ---------------------


#STEP-1: INSTALLING GIT JAVA-1.8.0 MAVEN 
yum install git java-1.8.0-openjdk maven -y

#STEP-2: GETTING THE REPO (jenkins.io --> download -- > redhat)
sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo
sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key

#STEP-3: DOWNLOAD JAVA11 AND JENKINS
sudo yum install java-17-amazon-corretto -y
yum install jenkins -y
update-alternatives --config java

#STEP-4: RESTARTING JENKINS (when we download service it will on stopped state)
systemctl start jenkins.service
systemctl status jenkins.service


CONNECT:
copy-public-ip:8080 (browser)
cat /var/lib/jenkins/secrets/initialAdminPassword (server)
paster password on browser -- > installing plugins --- > user details -- > start


JOB: it is used to perform task.
to do any work or task in jenkins we need to create a job.

to run commands we need to select execute shell on build steps.

build now: to run job
workspace: place where our job outputs will store

CREATING A JOB:
NEW ITEM -- > NAME: ABC -- > FREESTYLE -- > OK -- > SCM -- > GIT -- > REPOURL: https://github.com/devopsbyraham/jenkins-java-project.git -- >Build Steps -- > ADD Build Steps -- > Execute shell -- > mvn clean package -- > save -- > build now

WORKSPACE: where your job output is going to be stored
Default: /var/lib/jenkins/workspace


=============================================================================
SETTING CI SERVER USING SCRIPT:

CREATE A SERVER
sudo -i 

vim jenkins.sh

#STEP-1: INSTALLING GIT JAVA-1.8.0 MAVEN 
yum install git java-1.8.0-openjdk maven -y

#STEP-2: GETTING THE REPO (jenkins.io --> download -- > redhat)
sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo
sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key

#STEP-3: DOWNLOAD JAVA11 AND JENKINS
amazon-linux-extras install java-openjdk11 -y
yum install jenkins -y
update-alternatives --config java

#STEP-4: RESTARTING JENKINS (when we download service it will on stopped state)
systemctl start jenkins.service
systemctl status jenkins.service

:wq

To run script: sh jenkins.sh

To execute commands on jenkins use execute shell under build steps.

VARIABLES:
it is used to store values that are going to change frequently.
ex: date, season -----

TYPES OF VARIABLES IN JENKINS:
1. USER DEFINED
2. JENKINS ENV

1. USER DEFINED VARIABLES: these are defined by user
a. Local Variable: Variable will work inside of job.
will be working for only single job.

NEW ITEM -- > NAME: ABC -- > FREESTYLE -- > OK -- > BUILD -- >EXECUTE SHELL

name=raham
echo "hai all my name is $name, $name is from hyderabad, $name is teaching devops"


b. Global Variable: Variable will work outside of job.
will be working for multiple job.


Dashboard -- > Manage Jenkins -- > System -- > Global properties  -- > Environment variables -- > add : Name: name value: raham -- > save 

NOTE: while defining variables spaces will not be given.
local variables will be high priority.

Limitation: some values cant be defined by user because these values will change build by build.
ex: build number, time, name, url -----

2. JENKINS ENV VARIABLES: these are defined by Jenkins itself.
a. these variables can be change from build to build.
b. these variables will be on upper case.
c. these variables can be defined only once.

echo "the build number is $BUILD_NUMBER, the job name is $JOB_NAME"

printenv: gives all env vars of jenkins



find / -name jenkins.service
find command used to find the path of a file.

ADMIN TASKS:
1. CHANGING PORT NUMBER OF JENKINS:

vim /usr/lib/systemd/system/jenkins.service
line-70: 8080=8090 -- > save and exit
systemctl daemon-reload
systemctl restart jenkins.service

When we chnage configuration of any service we need to restart.

2. PASSWORDLESS LOGIN

vim /var/lib/jenkins/config.xml
line-10: true=false
systemctl restart jenkins.service

now check the jenkins dashboard it wont ask password


3. HOW TO RESOLVE THE ISSUE IF JENKINS SERVER CRASHED ?
stop the jenkins server and start it 
systemctl restart jenkins

When we stop server the services will be also stopped
so we want to restart them 

systemctl stop jenkins.service
systemctl restart Jenkins.service


BUILD EXECUTORS & PARALLEL BUILDS:
Jenkins will run the jobs sequentially (one by one)
if i want to run multiple builds at same time we can configure like this

job -- > configure -- > Execute concurrent builds if necessary -- > save -- > build now 2 times
now we can see 2 jobs will be running on same time.

BUILD EXECUTORS: max number of builds we can run
build-executor status -- > Built-In Node -- > Configure -- > 2 - 5 -- >save
now build 5 times

CRON JOB: We can schedule the jobs that need to be run at particular intervals.
here we use cron syntax
cron syntax has * * * * *
each * is separated by space

*	: minutes
*	: hours
*	: date
*	: month
*	: day of week (sun=0, mon=1 ----)

16:49 wed 19 2024

49 16 19 6 3
create a ci job -- > Build Triggers -- > Build periodically -- > * * * * * -- > save

CRONTAB-GENERATOR: https://crontab-generator.org/

limitation: it will not check the code is changed or not.


POLL SCM: 
in pollscm we will set time limit for the jobs.
if dev commit the code it will wait until the time is done.
in given time if we have any changes on code it will generate a build

create a ci job -- > Build Triggers -- > poll scm -- > * * * * * -- > save
commit the changes in GitHub then wait for 1 min.

LIMITATION:
1. in pollscm, we need to wait for the time we set.
2. we will get the last commit only.

WEBHOOK: it will trigger build the moment we change the code.
here we need not to wait for the build.

repo -- > settings -- > webhooks -- > add webhook -- > Payload URL (jenkins url) -- > http://35.180.46.134:8080/github-webhook/  -- > Content type -- > application/json -- > add

create ci job -- > Build Triggers: GitHub hook trigger for GITScm polling -- > save


BUILD SCRIPTS: to make jenkins builds from remote loc using script/
give token 
give url on other browser.

THROTTLE BUILD:

To restrict the builds in a certain time or intervals.
if we dont restrict due to immediate builds jenkins might crashdown.

by default jenkins will not do concurrent builds.
we need to enable this option in configuration.

Execute concurrent builds if necessary -- > tick it

create a ci job -- > configure -- > Throttle builds -- > Number of builds: 3 -- > time period : hours -- > save

now it will take 20  mins of gap for each build.
===============================================================================


NOTE: if we stop server then services inside server also going to stop.
chkconfig jenkins on
the above command will restart the jenkins service all the time.

PIPELINE: STEP BY EXECUTION OF A PROCESS
SERIES OF EVENTS INTERLINKED WITH EACH OTHER.
code -- > build -- > test -- > artifact -- > deployment
IF PIPELINE FAILS ON STAGE-1 IT WONT GO FOR STAGE-2.


why to use ?
to automate the work.
to have clarity about the stage.

TYPES:
1. DECLARATIVE
2. SCRIPTED

pipeline syntax:
to write the pipeline we use DSL.
We use Groovy Script for jenkins Pipeline.
it consists of blocks that include stages.
it includes () & {} braces. 


SHORTCUT: PASSS

P	: PIPELINE
A	: AGENT
S	: STAGES
S	: STAGE
S	: STEPS 


SINGLE STAGE: this pipeline will have only one stage.

EX-1:
pipeline {
    agent any 
    
    stages {
        stage('abc') {
            steps {
               sh 'touch file1'
            }
        }
    }
}

EX-2:
pipeline {
    agent any 
    
    stages {
        stage('raham') {
            steps {
                sh 'touch file2'
            }
        }
    }
}

MULTI STAGE: this pipeline will have more than one stage.

pipeline {
    agent any
    
    stages {
        stage ('two') {
            steps {
                sh 'lsblk'
            }
        }
        stage ('three') {
            steps {
                sh 'lscpu'
            }
        }
        stage ('four') {
            steps {
                sh 'lsmem'
            }
        }
    }
}


CI PIPELINE:

CODE + BUILD + TEST + ARTIFACT

pipeline {
    agent any
    
    stages {
        stage ('checkout') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
            }
        }
        stage ('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage ('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage ('artifact') {
            steps {
                sh 'mvn package'
            }
        }
    }
}


PIPELINE AS A CODE: Running more than one command/action inside a single stage.
to reduce the length of the code.
to save the time.

pipeline {
    agent any
    
    stages {
        stage ('checkout') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
                sh 'mvn compile'
                sh 'mvn test'
                sh 'mvn package'
            }
        }
    }
}

MULTI STAGE PIPELINE AS A CODE: Running more than one command/action in multiple stages.


pipeline {
    agent any
    
    stages {
        stage ('one') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
                sh 'mvn compile'
            }
        }
        stage ('two') {
            steps {
                sh 'mvn test'
                sh 'mvn package'
            }
        }
    }
}

PAAC OVER SINGLE SHELL: Running all the shell commands on a single shell.

pipeline {
    agent any
    
    stages {
        stage ('one') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
                sh '''
                mvn compile
                mvn test
                mvn package
                '''
            }
        }
    }
}


INPUT PARAMETERS: BASED ON USER INPUT THE PIPELINE IS GOING TO EXECUTE.

pipeline {
    agent any
    
    stages {
        stage ('checkout') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
            }
        }
        stage ('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage ('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage ('artifact') {
            steps {
                sh 'mvn package'
            }
        }
        stage ('deploy') {
            input {
                message "is your inputs correct ?"
                ok "yes"
            }
            steps {
                echo "my code is deployed"
            }
        }
    }
}
NOTE: In real time providing manual approval is best practise for Jenkins pipelines.

NOTE: if we have syntax issue none of the stages will execute.

DIFF BLW SCRIPTED VS DECLARATVE

SCRIPTED: 	DECLARATIVE: 
SHORT   	LONG
NO STAGES       IT HAS STAGES
START NODE      START WITH PIPELINE
======================================================================

MASTER AND SLAVE:
it is used to distribute the builds.
it reduce the load on jenkins server.
communication blw master and slave is ssh.
Here we need to install agent (java-11).
slave can use any platform.
label = way of assingning work for slave.

SETUP:
#STEP-1 : Create a server and install java-11
amazon-linux-extras install java-openjdk11 -y

#STEP-2: SETUP THE SLAVE SERVER
Dashboard -- > Manage Jenkins -- > Nodes & Clouds -- > New node -- > nodename: abc -- > permanaent agent -- > save 

CONFIGURATION OF SALVE:

Number of executors : 3 #Number of Parallel builds
Remote root directory : /tmp #The place where our output is stored on slave sever.
Labels : swiggy #place the op in a particular slave
useage: last option
Launch method : last option 
Host: (your privte ip)
Credentials -- > add -- >jenkins -- > Kind : ssh username with privatekey -- > username: ec2-user 
privatekey : pemfile of server -- > save -- > 
Host Key Verification Strategy: last option

DASHBOARD -- > JOB -- > CONFIGURE -- > RESTRTICT WHERE THIS JOB RUN -- > LABEL: SLAVE1 -- > SAVE

BUILD FAILS -- > WHY -- > WE NEED TO INSTALL PACKAGES
yum install git java-1.8.0-openjdk maven -y


pipeline {
    agent {
        label 'raham'
    }
    
    stages {
        stage('code') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
            }
        }
        stage('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn package'
            }
        }
    }
}


============================================================

NEXUS:
Its an Artifactory storage service.
used to store artifacts on repo. (.JAR, .WAR, .EAR)
Nexus server -- > Repo -- > Artifact
we can use this server to rollback in real time.
it req t2.medium 
nexus uses java-1.8.0
PORT: 8081

ALTERTAVIVES: JFROG, S3, -----

SETU SCIPT:
https://github.com/RAHAMSHAIK007/all-setups.git

STEPS: signin -- > username: admin & passowrd: /app/sonatype-work/nexus3/admin.password -- > next -- > set passowrd -- > disable ananamous access -- > save

CREATING REPO:
settings symbol -- > repositories -- > new -- > maven2(hosted) -- > name -- > save

NOTE: to integrate any tool with Jenkins we need to download the plugin.

NEXUS INTEGRATION TO PIPELINE:
1. Download the plugin (Nexus Artifact Uploader)
Manage Jenkins -- > plugins -- > Available Plugins -- > Nexus Artifact Uploader -- > install.
2. Configure it to pipeline by using pipeline syntax

NOTE: All the information will be available on pom.xml file.


PIPELINE:

pipeline {
    agent any
    
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
            }
        }
        stage('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn package'
            }
        }
        stage('Artifact upload') {
            steps {
                nexusArtifactUploader artifacts: [[artifactId: 'NETFLIX', classifier: '', file: 'target/NETFLIX-1.2.2.war', type: '.war']], credentialsId: '968c23dd-b648-4f15-91bf-7d76981a1218', groupId: 'in.RAHAM', nexusUrl: '100.25.197.110:8081', nexusVersion: 'nexus3', protocol: 'http', repository: 'netflix', version: '1.2.2'
            }
        }
    }
}


TOMCAT:

WEBSITE: FRONTEND -- > DB IS OPT
WEBAPP: FRONTEND + BACKEND -- > DB IS MANDATORY

ITS A WEB APPLICATION SERVER USED TO DEPLOY JAVA APPLICATIONS.
AGENT: JAVA-11
PORT: 8080
WE CAN DEPLOY OUR ARTIFACTS.
ITS FREE AND OPENSOURCE
IT IS WRITTEN ON JAVA LANGUAGE.
YEAR: 1999 

war : tomcat/webapps
jar : tomcat/lib

ALTERNATIVES: NGINX, IIS, WEBSPHERE, JBOSS, GLASSFISH


SETUP: CREATE A NEW SERVER
INSTALL JAVA: amazon-linux-extras install java-openjdk11 -y

STEP-1: DOWNLOAD TOMCAT (dlcdn.apache.org)
wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.87/bin/apache-tomcat-9.0.89.tar.gz

STEP-2: EXTRACT THE FILES
tar -zxvf apache-tomcat-9.0.89.tar.gz

STEP-3: CONFIGURE USER, PASSWORD & ROLES
vim apache-tomcat-9.0.89/conf/tomcat-users.xml

 56   <role rolename="manager-gui"/>
 57   <role rolename="manager-script"/>
 58   <user username="tomcat" password="raham123" roles="manager-gui, manager-script"/>

STEP-4: DELETE LINE 21 AND 22
vim apache-tomcat-9.0.89/webapps/manager/META-INF/context.xml

STEP-5: STARTING TOMCAT
sh apache-tomcat-9.0.89/bin/startup.sh

CONNECTION:
COPY PUBLIC IP:8080 
manager apps -- > username: tomcat & password: raham123



ADMIN ACTIVITIES:

REAL SCENARIOS:
port change:
vim /root/apache-tomcat-9.0.80/conf/server.xml (line 69)
sh /root/apache-tomcat-9.0.80/bin/shutdown.sh
sh /root/apache-tomcat-9.0.80/bin/startup.sh

passowrd:
vim apache-tomcat-9.0.80/conf/tomcat-users.xml
sh /root/apache-tomcat-9.0.80/bin/shutdown.sh
sh /root/apache-tomcat-9.0.80/bin/startup.sh

checking logs:
tail -100f apache-tomcat-9.0.83/logs/catalina.out


RBAC:

RBAC: ROLE BASE ACCESS CONTROL.
TO restrict the user PERMISSIONS in jenkins.

suresh	= fresher
raham	= exp 

STEP-1: USER CREATION
manage jenkins -- > users -- > create users -- > suresh: fresher 

STEP-2: PLUGIN DOWNLOADING
Dashboard
Manage Jenkins
Plugins
Available plugin
Role-based Authorization Strategy  

STEP-3: CONFIGURE THE PLUGIN
Dashboard
Manage Jenkins
Security
Authorization 
Role-based  Strategy  
SAVE

STEP-4: MANAGE AND ASSIGN USERS
manage roles -- > add -- > fresher & exp -- > fresher: overall read & exp: admin -- > save
assign roles -- > add user -- > rajesh: fresher -- > save



LINKED JOBS:
ONE JOB IS INKED WITH ANOTHER JOB
IF WE BUILD JOB-1 AUTOMATICALLY JOB-2 IS GOING TO BUILD.

=========================================================





Automated: Deployment, Installation
Non-Automated: Server
To perform end-to-end automation we can use Ansible.
Creating servers
configure servers
deployment application on servers

ANSIBLE:
its a Configuration Management Tool.
Ansible is used to manage and work with multiple servers together.
its a free and Opensource.
Configuration: Hardware and Software 
Management: Pkgs update, installing, remove ----
it is used to automate the entire deployment process on multiple servers.
We install Python on Ansible.
we use a key-value format for the playbooks.

PLAYBOOK:
create servers
install packages & software
deploy apps
---------



Jenkins = pipeline = groovy
ansible = playbooks = yaml


HISTORY:
in 2012 dev called Maichel Dehaan who developed ansible.
After few years RedHat taken the ansible.
it is platform-independent & will work on all linux flavours.


ARCHITECTURE:
PLAYBOOK: its a file which consist of code
INVENTORY: its a file which consist ip of nodes
SSH: used to connect with nodes
Ansible is Agent less.
Means no need to install any software on worker nodes.

SETUP: 
CREATE 5 SERVERS [1=ANSIBLE, 2=DEV, 2=TEST]

EXECUTE THE BELOW COMMANDS ON ALL SERVERS:
sudo -i
hostnamectl set-hostname ansible/dev-1/dev-2/test-1/test-2
sudo -i

passwd root  -- > to login to other servers
vim /etc/ssh/sshd_config (38 & 61 uncomment both lines) 
systemctl restart sshd
systemctl status sshd
hostname -i

THE BELOW STEPS NEED TO BE RUN ON ANSIBLE SERVER:

amazon-linux-extras install ansible2 -y
yum install python3 python-pip python-dlevel -y

vim /etc/ansible/hosts
# Ex 1: Ungrouped hosts, specify before any group headers.
[dev]
172.31.20.40
172.31.21.25
[test]
172.31.31.77
172.31.22.114

ssh-keygen -- > enter 4 times 
ssh-copy-id root@private ip of dev-1 -- > yes -- > password -- > ssh private ip -- > ctrl d
ssh-copy-id root@private ip of dev-2 -- > yes -- > password -- > ssh private ip -- > ctrl d
ssh-copy-id root@private ip of test-1 -- > yes -- > password -- > ssh private ip -- > ctrl d
ssh-copy-id root@private ip of test-2 -- > yes -- > password -- > ssh private ip -- > ctrl d

ansible -m ping all : To check worker node connection with ansible server.

1. ADHOC COMMANDS:
these are simple Linux commands. 
these are used for temp works.
these commands will be over ridden.

ansible all -a "yum install git -y"
ansible all -a "yum install maven -y"
ansible all -a "mvn --version"
ansible all -a "touch file1"
ansible all -a "touch raham.txt"
ansible all -a "ls"
ansible all -a "yum install httpd -y"
ansible all -a "systemctl status httpd"
ansible all -a "systemctl start httpd"
ansible all -a "user add raham"
ansible all -a "cat /etc/passwd"
ansible all -a "yum remove git* maven* httpd* -y"


HISTORY:

    1  amazon-linux-extras install ansible2 -y
    2  yum install python3 python-pip python-dlevel -y
    3  vim /etc/ansible/hosts
    4  vim /etc/ansible/hosts o
    5  vim /etc/ansible/hosts
    6  ssh-keygen
    7  ll .ssh/
    8  ssh-copy-id root@172.31.23.99
    9  ssh 172.31.23.99
   10  ssh-copy-id root@172.31.29.194
   11  ssh 172.31.29.194
   12  ssh-copy-id root@172.31.18.78
   13  ssh 172.31.18.78
   14  ssh-copy-id root@172.31.21.219
   15  ssh 172.31.21.219
   16  ansible all -m ping
   17  ansible -m ping all
   18  ansible all -a "touch file1"
   19  ansible all -a "ls"
   20  ansible all -a "yum install git -y"
   21  ansible all -a "git -v"
   22* ansible dev -a "yum install maven -y"
   23  ansible all -a "mvn -v"
   24  ansible dev -a "yum install tree -y"
   25  ansible all -a "tree -v"
   26  ansible all -a "useradd raham"
   27  ansible all -a "cat /etc/passwd"
   28  ansible all -a "yum install httpd -y"
   29  ansible all -a "systemctl status httpd"
   30  ansible all -a "systemctl start httpd"
   31  ansible all -a "systemctl status httpd"
   32  cat /etc/ansible/ansible.cfg
   33  vim /etc/ssh/sshd_config
   34  history
============================================================================

2. MODULES:
its a key-value pair.
modules are reusable.
we can use different modules for different purposes.
module flag is -m 

ansible all -m yum -a "name=git state=present"
ansible all -m yum -a "name=maven state=present"
ansible all -m yum -a "name=maven state=present"	[present=installed]
ansible all -m service -a "name=httpd state=started"	[started=restart]
ansible all -m service -a "name=httpd state=stopped"	[stopped=stop]
ansible all -m yum -a "name=http state=absent"		[absent=uninstall]
ansible all -m user -a "name=vikram state=present"
ansible all -m user -a "name=vikram state=absent"
ansible all -m copy -a "src=raham.txt dest=/tmp"

3. PLAYBOOKS:
playbooks used to execute multiple modules.
we can reuse the playbook multiple times.
in real time we use a playbook to automate our work.
for deployment, pkg installation, Server Creation ----
here we use key-value pairs.
Key-Value can also be called as Dictionary.
ansible-playbook will be written on YAML syntax.
YAML = YET ANOTHER MARKUP LANGUAGE
extension for playbook is .yml or .yaml
playbook start with --- and end with ... (opt)


EX-1:

- hosts: all
  tasks:
    - name: installing git
      yum: name=git state=present

    - name: installing httpd
      yum: name=httpd state=present

    - name: starting httpd
      service: name=httpd state=started

    - name: create user
      user: name=jayanth state=present

    - name: copy a file
      copy: src=index.html dest=/root

    

TO EXECUTE: ansible-playbook playbok.yml

Gather facts: it will get information of worker nodes
its by default task performed by ansible.

ok=total number of tasks
changed= no.of tasks successfully executed

EX-2:
 hosts: all
  ignore_errors: true
  tasks:
    - name: installing git
      yum: name=git state=absent

    - name: installing httpd
      yum: name=httpd state=absent

    - name: starting httpd
      service: name=httpd state=started

    - name: create users
      user: name=pushpa state=absent

    - name: copying a file
      copy: src=raham.txt dest=/root



SETUP MODULE: used to print the complete info of worker nodes
ansible all -m setup 

ansible all -m setup  | grep -i family
ansible all -m setup  | grep -i pkg
ansible all -m setup  | grep -i cores

HISTORY:
   42  ansible all -m ping
   43  ansible all -a "yum remove git* maven* httpd* tree* -y"
   44  ansible all -m ping
   45  sudo -i
   46  ansible all -a "git -v"
   47  ansible all -m yum -a "name=git state=present"
   48  ansible all -a "git -v"
   49  ansible all -m yum -a "name=maven state=present"
   50  ansible all -m yum -a "name=httpd state=present"
   51  ansible all -a "httpd -v"
   52  ansible all -m yum -a "name=httpd state=started"
   53  ansible all -m service -a "name=httpd state=started"
   54  ansible all -a "systemctl status httpd"
   55  ansible all -m service -a "name=httpd state=stopped"
   56  ansible all -a "systemctl status httpd"
   57  ansible all -m yum -a "name=httpd state=latest"
   58  ansible all -m yum -a "name=httpd state=absent"
   59  ansible all -m user -a "name=vijay state=present"
   60  ansible all -a "cat /etc/passwd"
   61  ansible all -m user -a "name=revii state=present"
   62  ansible all -a "cat /etc/passwd"
   63  ll
   64  vim raham.txt
   65  ll
   66  ansible all -m copy -a "src=raham.txt dest=/tmp"
   67  ansible all -a "ls"
   68  ansible all -a "ls /ymp"
   69  ansible all -a "ls /tmp"
   70  ansible all -a "yum remove maven* git* httpd* -y"
   71  vim raham.yml
   72  ansible-playbook raham.yml
   73  vim p
   74  vim raham.yml
   75  cat raham.yml
   76  sed -i 's/present/absent/' raham.yml
   77  cat raham.yml
   78  ansible-playbook raham.yml
   79  vim raham.yml
   80  ansible-playbook raham.yml
   81  cat raham.
   82  cat raham.yml
   83  ansible all -m setup
   84  ansible all -m setup | grep -i cpu
   85  ansible all -m setup | grep -i mem
   86  ansible all -m setup | grep -i node
   87  cat raham.yml
   88  history


=========================================================



TAGS: by deafult ansible will execute all tasks sequentially in a playbook.
we can use tags to execute a specific tasks or to skip a specific tasks.


EX-1:

- hosts: all
  ignore_errors: yes
  tasks:
    - name: installing git
      yum: name=git state=present
      tags: a

    - name: installing httpd
      yum: name=httpd state=present
      tags: b

    - name: starting httpd
      service: name=httpd state=started
      tags: c

    - name: create a user
      user: name=kohli state=present
      tags: d

    - name: copy a file
      copy: src=index.html dest=/tmp
      tags: e

SINGLE TAG: ansible-playbook raham.yml --tags d
MULTI TAGS: ansible-playbook raham.yml --tags b,c

EX-2:

- hosts: all
  ignore_errors: yes
  tasks:
    - name: uninstalling git
      yum: name=git* state=absent
      tags: a

    - name: uninstalling httpd
      yum: name=httpd state=absent
      tags: b

    - name: starting httpd
      service: name=httpd state=started
      tags: c

    - name: delete a user
      user: name=kohli state=absent
      tags: d

    - name: copy a file
      copy: src=index.html dest=/tmp
      tags: e

SKIP A SINGLE TASK: ansible-playbook raham.yml --skip-tags "c"
SKIP MULTIPLE TASK: ansible-playbook raham.yml --skip-tags "a,c"

VARIABLES:

STATIC VARS: we can define these vars inside the playbook and use for multiple times, once a variable is defined here it will not change untill we change.


- hosts: all
  vars:
    a: maven
    b: httpd
  tasks:
    - name: installing maven
      yum: name={{a}} state=present
    - name: installing httpd
      yum: name={{b}} state=present

TO EXECUTE: ansible-playbook playbbok.yml

DYNAMIC VARS: therse vars will be defined outside the playbook and these will change as per our requirments.

- hosts: all
  vars:
  tasks:
    - name: installing maven
      yum: name={{a}} state=absent
    - name: installing httpd
      yum: name={{b}} state=absent


ansible-playbook raham.yml --extra-vars "a=docker b=httpd"


LOOPS: We can use loops to reduce the length of the code for the playbook

- hosts: all
  tasks:
    - name: installing pkg-1
      yum: name={{item}} state=present
      with_items:
        - git
        - java-1.8.0-openjdk
        - maven
        - docker
        - httpd


ansible all -a "git -v"
ansible all -a "java -v"
ansible all -a "maven -v"
ansible all -a "docker -v"
ansible all -a "httpd -v"


- hosts: all
  tasks:
    - name: installing pkg-1
      yum: name={{item}} state=absent
      with_items:
        - git
        - java-1.8.0-openjdk
        - maven
        - docker
        - httpd

ansible all -a "git -v"
ansible all -a "java -v"
ansible all -a "maven -v"
ansible all -a "docker -v"
ansible all -a "httpd -v"

EX-2:

- hosts: all
  tasks:
    - name: creating users
      user: name={{item}} state=present
      with_items:
        - ravi
        - shiva
        - rajesh
        - shivani
        - luckyy


- hosts: all
  tasks:
    - name: creating users
      user: name={{item}} state=absent
      with_items:
        - ravi
        - shiva
        - rajesh
        - shivani
        - luckyy


HANDLERS:
when we have two tasks in a single playbook if task 1 is depending upon task 2 so then we can use the concept called handlers .
once task one is executed successfully it will notify task 2 to perform the operation. 
the name of the notify and the name of the task two must be same.


- hosts: all
  tasks:
    - name: installing httpd
      yum: name=httpd state=present
      notify: starting httpd
  handlers:
    - name: starting httpd
      service: name=httpd state=started

sed -i 's/present/absent/g' raham.yml

- hosts: all
  tasks:
    - name: installing httpd
      yum: name=httpd state=absent
      notify: starting httpd
  handlers:
    - name: starting httpd
      service: name=httpd state=started

SHELL VS COMMAND VS RAW:

- hosts: all
  tasks:
    - name: installing maven
      shell: yum install maven -y

    - name: installing httpd
      command: yum install httpd -y

    - name: installing docker
      raw: yum install docker -y

raw >> command >> shell.


HISTORY:  89  ll
   90  rm -rf raham.txt
   91  cat raham.yml
   92  rm -rf raham.yml
   93  vim raham.yml
   94  touch index.html
   95  cat raham.yml
   96  ansible-playbook raham.yml --tags e
   97  ansible-playbook raham.yml --tags b,c
   98  ansible all -a "yum remove httpd* -y"
   99  cat raham.yml
  100  ansible-playbook raham.yml --skip-tags "e"
  101  ansible-playbook raham.yml --skip-tags "e,a"
  102  cat raham.yml
  103  sed -i 's/present/absent/; s/installing/uninstalling/' raham.yml
  104  cat raham.yml
  105  ansible-playbook raham.yml
  106  vim raham.yml
  107  ansible-playbook raham.yml
  108  vim raham.yml
  109  cat raham.yml
  110  sed -i 's/present/absent/' raham.yml
  111  cat raham.yml
  112  ansible-playbook raham.yml --extra-vars "a=maven b=httpd"
  113  vim raham.yml
  114  ansible-playbook raham.yml
  115  vim raham.yml
  116  ansible-playbook raham.yml
  117  ansible all -a "mvn -v"
  118  ansible all -a "httpd -v"
  119  ansible all -a "git -v"
  120  ansible all -a "docker -v"
  121  ansible all -a "tree -v"
  122  vim raham.yml
  123  ansible-playbook raham.yml
  124  ansible all -a "mvn -v"
  125  ansible all -a "httpd -v"
  126  ansible all -a "git -v"
  127  ansible all -a "docker -v"
  128  ansible all -a "tree -v"
  129  vim raham.yml
  130  ansible-playbook raham.yml
  131  ansible all -a "cat /etc/passwd"
  132  vim raham.yml
  133  ansible-playbook raham.yml
  134  vim raham.yml
  135  ansible-playbook raham.yml
  136  vim raham.yml
  137  sed -i 's/present/absent/' raham.yml
  138  ansible-playbook raham.yml
  139  vim raham.yml
  140  ansible-playbook raham.yml
  141  cat raham.yml
  142  sed -i 's/install/remove/' raham.yml
  143  ansible-playbook raham.yml
  144  vim raham.yml
  145  touch index.html
  146  ansible-playbook raham.yml
  147  vim raham.yml
  148  ansible-playbook raham.yml
  149  sed -i 's/absent/present/' raham.yml
  150  ansible-playbook raham.yml
  151  cat raham.yml
  152  history
=============================================================================

CONDITIONS:
CLUSTER: Group of servers
HOMOGENIUS: all servers have having same OS and flavour.
HETROGENIUS: all servers have different OS and flavour.

used to execute this module when we have different Clusters.

RedHat=yum
Ubuntu=apt

- hosts: all
  tasks:
    - name: installing git on RedHat
      yum: name=git state=present
      when: ansible_os_family == "RedHat"

    - name: installing git on Debian
      apt: name=git state=present
      when: ansible_os_family == "Debian"



L	: LINUX
A	: APACHE
M	: MYSQL
P	: PYTHON


- hosts: all
  tasks:
    - name: installing apache
      yum: name=httpd state=present

    - name: installing mysql
      yum: name=mysql state=present

    - name: installing python
      yum: name=python3 state=present


 ansible all -a "httpd --version"
 ansible all -a "python3 --version"
 ansible all -a "mysql --version"


DEBUG: to print the messages from a playbook.

- hosts: all
  tasks:
    - name: printing a msg
      debug:
        msg: hai all welcome to my session



NAME	: ansible_nodename
FAMILY  : ansible_os_family
PKG	: ansible_pkg_mgr
CPU	: ansible_processor_cores
MEM	: ansible_memtotal_mb
FREE	: ansible_memfree_mb


- hosts: all
  tasks:
    - name: print a msg
      debug:
        msg: "my node name is: {{ansible_nodename}}, the os is: {{ansible_os_family}}, the package manager is: {{ansible_pkg_mgr}}, total cpus is: {{ansible_processor_cores}}, the total ram: {{ansible_memtotal_mb}}, free ram is: {{ansible_memfree_mb}}"


JINJA2 TEMPLATE: used to get the customized op, here its a text file which can extract the variables and these values will change as per time.

LOOKUPS: this module used to get data from files, db and key values

- hosts: dev
  vars:
    a: "{{lookup('file', '/root/creds.txt') }}"
  tasks:
    - debug:
        msg: "hai my user name is {{a}}"

cat creds.txt
user=raham
password=test123


STRATAGIES: Way of executing the playbook.

LINEAR: execute tasks sequentially 
if task-1 is executed on server-1 it will wait till task-2 execution
FREE: execute all tasks on all node at same time
if task-1 is executed on server-1 it wont wait till task-2 execution
ROLLING:
BATCH:

====================================================================================

 roles
     pkgs
      tasks
          main.yml
     users
      tasks
          main.yml
     webserver
         tasks
             main.yml


ROLES:
roles is a way of organizing playbooks in a structured format.
main purpose of roles is to encapsulate the data.
we can reuse the roles multiple times.
length of the playbook is decreased.
it contains on vars, templates, task -----
in real time we use roles for our daily activities.
yum install tree -y

mkdir playbooks
cd playbooks/

mkdir -p roles/pkgs/tasks
vim roles/pkgs/tasks/main.yml

- name: installing pkgs
  yum: name=git state=present
- name: install maven
  yum: name=maven state=present
- name: installing docker
  yum: name=docker state=present

mkdir -p roles/users/tasks
vim roles/users/tasks/main.yml

- name: create users
  user: name={{item}} state=present
  with_items:
    - uday
    - naveen
    - rohit
    - lokesh
    - saipallavi
    - supriya

mkdir -p roles/webserver/tasks
vim roles/web/tasks/main.yml

- name: installing httpd
  yum: name=httpd state=present

- name: starting httpd
  service: name=httpd state=started

cat master.yml

- hosts: all
  roles:
    - pkgs
    - users
    - webserver

find . -type f -exec sed -i 's/present/absent/g' {} \;


ANSIBLE GALAXY:

Ansible Galaxy is a  website where users can share roles and to a command-line tool for installing, creating, and managing roles.
Ansible Galaxy gives greater visibility to one of Ansible's most exciting features, such as application installation or reusable roles for server configuration. 
Lots of people share roles in the Ansible Galaxy.
Ansible roles consist of many playbooks, which is a way to group multiple tasks into one container to do the automation in a very effective manner with clean, directory structures.

ANSIBLE VAULT:
it is used to encrypt the files, playbooks ----
Technique: AES256 (USED BY FACEBOOK, AWS)
vault will store our data very safely and securely.
if we want to access any data which is in the vault we need to give a password.
Note: we can restrict the users to access the playbook also.

cat creds.txt
user=raham
passowrd=test123

ansible-vault create creds1.txt		: to create a vault
ansible-vault edit creds1.txt		: to edit a vault
ansible-vault rekey creds1.txt		: to change password for a vault
ansible-vault decrypt creds1.txt	: to decrypt the content	
ansible-vault encrypt creds1.txt	: to encrypt the content	
ansible-vault view creds1.txt		: to show the content without decrypt

PIP: its a pkg manager used to install python libs/modules

Redhat: yum
ubuntu: apt
python: pip

- hosts: all
  tasks:
    - name: install pip
      yum: name=pip state=present

    - name: installing NumPy
      pip: name=NumPy state=present

    - name: installing Pandas
      pip: name=Pandas state=present




LOOKUPS: this module used to get data from files, db and key values

- hosts: dev
  vars:
    a: "{{lookup('file', '/root/creds.txt') }}"
  tasks:
    - debug:
        msg: "hai my user name is {{a}}"

cat creds.txt
user=raham
password=test123

JINJA2 TEMPLATE: used to get the customized op, here its a text file which can extract the variables and these values will change as per time.


ASYNCHRONOUS & POLLING ACTIONS:
for every task in  ansible we can set time limit
if the task is not performed in that time limit ansible will stop playbook execution
this is called as asynchronous and polling.

- hosts: all
  ignore_errors: yes
  tasks:
    - name: sleeping
      command: sleep 30
      async: 10
      poll: 20
    - name: install git
      yum: name=git state=present

WEB SERVER : TO SHOW THE APP : httpd  : 80  : /var/www/html
frontend code
APP SERVER : TO USE THE APP : Tomcat  : 8080  : tomcat/webapps
frontend code + backend code

COMMENTS:
in YAML syntax we have only single line comment we don't have multiline comments for it by default

DRY RUN:
Ansible Dry Run or Ansible Check mode feature is to validate your playbook before execution. 
If we execute the playbook with dry run it won't do any changes to the worker nodes.


TOMCAT SETUP FROM BELOW LINK:
https://github.com/RAHAMSHAIK007/all-setups.git

================================================================================

LINK FOR SCRIPTS & PLAYBOOKS : https://github.com/RAHAMSHAIK007/all-setups.git
LINK FOR PROJECT: https://github.com/devopsbyraham/jenkins-java-project.git

Install Jenkins on ansible server and connect to dashboard

INTEGRTAING ANSIBE WITH JENKINS:


34.226.196.178:8080/NETFLIX-1.2.2/

1. install ansible plugin
2. configure ansible tool 
manage jenkins -- > tools -- > ansible -- > name: ansible & Path to ansible executables directory: /usr/bin -- > save

3. SAMPLE STEP: ANSIBLE PLAYBOOK
Ansible tool: ansible -- > Playbook file path in workspace: /etc/ansible/playbook.yml -- > 
Inventory file path in workspace: /etc/ansible/hosts -- > SSH CREDS: give creds of ansible & worker nodes -- > Disable the host SSH key check -- > generate script

18.215.171.109:8080/

pipeline {
    agent any
    
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/devopsbyraham/jenkins-java-project.git'
            }
        }
        stage('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn package'
            }
        }
        stage('nexus upload') {
            steps {
                echo "artifact is uploaded to nexus"
            }
        }
        stage('deploy') {
            steps {
                ansiblePlaybook credentialsId: '959fbd45-dd0d-44d9-95e6-dc2c41c7c58e', disableHostKeyChecking: true, installation: 'ansible', inventory: '/etc/ansible/hosts', playbook: '/etc/ansible/playbook.yml', vaultTmpPath: ''
            }
        }
    }
}


cat playbook.yml
- hosts: all
  tasks:

    - name: task1
      copy:
        src: /var/lib/jenkins/workspace/pipeline/target/NETFLIX-1.2.2.war
        dest: /root/tomcat/webapps



=====================================================
CREATING EC2 FROM PLAYBOOK:

NOTE: If Ansible want to create an Ec2 instance in the cloud it need to have permission 
so to allocate the permission for our Ansible we can create IAM user

IAM -- > create user -- > name: Ansible -- > next -- > Attach policies directly -- > AdministratorAccess -- > next -- > create user 

Ansible -- > Security Credentials -- > Create access key -- > CLI -- > checkbox -- > create access key --> download .csv file

COME TO ANSIBLE SERVER:
aws configure -- > giving ansible user permissions to server

AWS Access Key ID [None]: xxxxxxxxxxxxx
AWS Secret Access Key [None]: xxxxxxxxxxxx
Default region name [None]: us-east-1
Default output format [None]: table

sudo pip install boto

- hosts: localhost
  tasks:
    - name: creating ec2 instance
      ec2:
        region: "us-east-1"
        count: 3
        image: "ami-04ff98ccbfa41c9ad"
        instance_type: "t2.micro"
        instance_tags:
          Name: "abc"


HOW TO ADD PARAMETERS:

PARAMETERS: Used to pass information for jobs


CHOICE: to pass single input at a time.
STRING: to pass multiple inputs at a time.
MULTI-LINE STRING: to pass multiple inputs on multiple lines at a time.
FILE: to pass the file as input.
BOOL: to pass input either yes or no.


This project is parameterized
Name: server 
Choices: 
dev
test
save



PIPELINE CODE:

pipeline {
    agent any
    
    stages {
        stage('checkout') {
            steps {
                git branch: '$branch', url: 'https://github.com/devopsbyraham/jenkins-java-project.git'
            }
        }
        stage('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('Artifact') {
            steps {
                sh 'mvn package'
            }
        }
        stage('Nexus') {
            steps {
                nexusArtifactUploader artifacts: [[artifactId: 'NETFLIX', classifier: '', file: 'target/NETFLIX-1.2.2.war', type: '.war']], credentialsId: 'f1071717-fb38-426b-aaaf-4d70d51d54f7', groupId: 'in.RAHAM', nexusUrl: '100.25.177.236:8081', nexusVersion: 'nexus3', protocol: 'http', repository: 'netflix', version: '1.2.2'
            }
        }
        stage('Deploy') {
            input {
                message "parameter check done ?"
                ok "yes"
            }
            steps {
                ansiblePlaybook credentialsId: '8918f57a-5dbf-4be3-b7d5-f3a43d5315a4', disableHostKeyChecking: true, installation: 'ansible', inventory: '/etc/ansible/hosts', limit: '$server', playbook: '/etc/ansible/deploy.yml', vaultTmpPath: ''
            }
        }
    }
}

====================================================================================

APPLICATION: Collection of services 

MONOLITHIC: multiple services are deployed on single server with single database.
MICRO SERVICES: multiple services are deployed on multiple servers with multiple database.

BASED ON USERS AND APP COMPLEXITY WE NEED TO SELECT THE ARCHITECTURE.

FACTORS AFFECTIONG FOR USING MICRO SERVICES:
F-1: COST 
F-2: MAINTAINANCE

CONTAINERS:
its same as a server/vm.
it will not have any operating system.
os will be on images.
(SERVER=AMI, CONTAINER=IMAGE)
its free of cost and can create multiple containers.

DOCKER: 
Its an free & opensource tool.
it is platform independent.
used to create, run & deploy applications on containers.
it is introduced on 2013 by solomenhykes & sebastian phal.
We used GO laguage to develope the docker.
here we write files on YAML.
before docker user faced lot of problems, but after docker there is no issues with the application.
Docker will use host resources (cpu, mem, n/w, os).
Docker can run on any OS but it natively supports Linux distributions.

CONTAINERIZATION:
Process of packing an application with its dependencies.
ex: PUBG

APP= PUBG & DEPENDECY = MAPS
APP= CAKE & DEPENDECY = KNIFE

os level of virtualization.

VIRTUALIZATION:
able to create resouce with our hardware properties.

ARCHITECTURE & COMPONENTS:
client: it will interact with user
user gives commands and it will be executed by docker client

daemon: manages the docker components(images, containers, volumes)

host: where we install docker (ex: linux, windows, macos)

Registry: manages the images.

ARCHITECTURE OF DOCKER:
yum install docker -y    #client
systemctl start docker	 #client,Engine
systemctl status docker


COMMANDS:
docker pull ubuntu	: pull ubuntu image
docker images		: to see list of images
docker run -it --name cont1 ubuntu : to create a container
-it (interactive) - to go inside a container
cat /etc/os-release	: to see os flavour


apt update -y	: to update 
redhat=yum
ubuntu=apt
without update we cant install any pkg in ubuntu


apt install git -y
apt install apache2 -y
service apache2 start
service apache2 status

docker p q		: to exit container
docker ps -a		: to list all containers
docker attach cont_name	: to go inside container
docker stop cont_name	: to stop container
docker start cont_name	: to start container
docker pause cont_name	: to pause container
docker unpause cont_name: to unpause container
docker inspect cont_name: to get complete info of a container
docker rm cont_name	: to delete a container

STOP: will wait to finish all process running inside container
KILL: wont wait to finish all process running inside container

================================================

OS LEVEL OF VIRTUALIZATION:

docker pull ubuntu
docker run -it --name cont1 ubuntu
apt update -y
apt install mysql-server apache2 python3 -y
touch file{1...5}
apache2 -v
mysql-server --version
python3 --version
ls

ctrl p q

docker commit cont1 raham:v1
docker run -it --name cont2 raham:v1
apache2 -v
mysql-server --version
python3 --version
ls


DOCKERFILE:
it is an automation way to create image.
here we use components to create image.
in Dockerfile D must be Capiatl.
Components also capital.
This Dockerfile will be Reuseable.
here we can create image directly without container help.
Name: Dockerfile

docker kill $(docker ps -qa)
docker rm $(docker ps -qa)
docker rmi -f $(docker images -qa)

COMPONENTS:

FROM		: used to base image
RUN		: used to run linux commands (During image creation)
CMD		: used to run linux commands (After container creation)
ENTRYPOINT	: high priority than cmd
COPY		: to copy local files to conatiner
ADD		: to copy internet files to conatiner
WORKDIR		: to open req directory
LABEL		: to add labels for docker images
ENV		: to set env variables (inside container)
ARGS		: to pass env variables (outside containers)
EXPOSE		: to give port number


EX-1:
FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y

docker build -t raham:v1 .
docker run -it --name cont1 raham:v1 

EX-2:
FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y
RUN apt install python3 -y
CMD apt install mysql-server -y


docker build -t raham:v2 .
docker run -it --name cont2 raham:v2

EX-3:
FROM ubuntu
COPY index.html /tmp
ADD http://dlcdn.apache.org/tomcat/tomcat-9/v9.0.89/bin/apache-tomcat-9.0.89.tar.gz /tmp

docker build -t raham:v3 .
docker run -it --name cont3 raham:v3

EX-4:

FROM ubuntu
COPY index.html /tmp
ADD http://dlcdn.apache.org/tomcat/tomcat-9/v9.0.89/bin/apache-tomcat-9.0.89.tar.gz /tmp
WORKDIR /tmp
LABEL author rahamshaik

docker build -t raham:v4 .
docker run -it --name cont4 raham:v4


EX-5:
FROM ubuntu
LABEL author rahamshaik
ENV client swiggy
ENV server appserver

docker build -t raham:v5 .
docker run -it --name cont5 raham:v5


NETFLIX-DEPLOYMENT:

yum install git -y
git clone https://github.com/RAHAMSHAIK007/netflix-clone.git
mv netflix-clone/*

 Dockerfile

FROM ubuntu
RUN apt update
RUN apt install apache2 -y
COPY * /var/www/html/
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]


docker build -t netflix:v1 .
docker run -it --name netflix1 -p 80:80 netflix:v1

=======================================================================================

vim Dockerfile

FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y
COPY index.html /var/www/html
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]

Index.html: take form w3 schools 

docker build -t movies:v1 .
docker run -itd --name movies -p 81:80 movies:v1

docker build -t train:v1 .
docker run -itd --name train -p 82:80 train:v1

docker build -t dth:v1 .
docker run -itd --name dth -p 83:80 dth:v1

docker build -t recharge:v1 .
docker run -itd --name recharge -p 84:80 recharge:v1

docker ps -a -q		: to list container ids
docker kill $(docker ps -a -q) : to kill all containers 
docker rm $(docker ps -a -q) : to remove all containers 

Note: In the above process all the containers are managed and created one by one in real time we manage all the continers at same time so for that purpose we are going to use the concept called Docker compose.



DOCKER COMPOSE:
It's a tool used to manage multiple containers in single host.
we can create, start, stop, and delete all containers together.
we write container information in a file called a compose file.
compose file is in YAML format.
inside the compose file we can give images, ports, and volumes info of containers.
we need to download this tool and use it.

INSTALLATION:
sudo curl -L "https://github.com/docker/compose/releases/download/1.29.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
ls /usr/local/bin/
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
docker-compose version


In Linux majorly you are having two type of commands first one is inbuilt commands which come with the operating system by default 
second one is download commands we are going to download with the help of yum, apt or Amazon Linux extras.

some commands we can download on binary files.

NOTE: linux will not give some commands, so to use them we need to download seperately
once a command is downloaded we need to move it to /usr/local/bin
because all the user-executed commands in linux will store in /usr/local/bin
executable permission need to execute the command



vim docker-compose.yml

version: '3.8'
services:
  movies:
    image: movies:v1
    ports:
      - "81:80"
  train:
    image: train:v1
    ports:
      - "82:80"
  dth:
    image: dth:v1
    ports:
      - "83:80"
  recharge:
    image: recharge:v1
    ports:
      - "84:80"

COMMANDS:
docker-compose up -d		: to create and start all containers
docker-compose stop		: to stop all containers
docker-compose start		: to start all containers
docker-compose kill		: to kill all containers
docker-compose rm		: to delete all containers
docker-compose down		: to stop and delete all containers
docker-compose pause		: to pause all containers
docker-compose unpause		: to unpause all containers
docker-compose ps -a		: to list the containers managed by compose file
docker-compose images		: to list the images managed by compose file
docker-compose logs		: to show logs of docker compose
docker-compose top		: to show the process of compose containers
docker-compose restart		: to restart all the compose containers
docker-compose scale train=10	: to scale the service


CHANGING THE DEFULT FILE:

by default the docker-compose wil support the following names
docker-compose.yml, docker-compose.yaml, compose.yml, compose.yaml

mv docker-compose.yml raham.yml
docker-compose up -d	: throws an error

docker-compose -f raham.yml up -d
docker-compose -f raham.yml ps
docker-compose -f raham.yml down


images we create on server.
these images will work on only this server.

git (local) -- > github (internet) = to access by others
image (local) -- > dockerhub (internet) = to access by others

Replace your username 

STEPS:
create dockerhub account
create a repo

docker tag movies:v1 vijaykumar444p/movies
docker login -- > username and password
docker push vijaykumar444p/movies


docker tag train:v1 vijaykumar444p/train
docker push vijaykumar444p/train


docker tag dth:v1 vijaykumar444p/dth
docker push vijaykumar444p/dth

docker tag recharge:v1 vijaykumar444p/recharge
docker push vijaykumar444p/recharge

docker rmi -f $(docker images -q)
docker pull vijaykumar444p/movies:latest



=================================================================

High Avaliabilty: more than one server
why: if one server got deleted then other server will gives the app

DOCKER SWARM:
its an orchestration tool for containers. 
used to manage multiple containers on multiple servers.
here we create a cluster (group of servers).
in that clutser we can create same container on multiple servers.
here we have the manager node and worker node.
manager node will create & distribute the container to worker nodes.
worker node's main purpose is to maintain the container.
without docker engine we cant create the cluster.
Port: 2377
worker node will join on cluster by using a token.
manager node will give the token.


http://54.172.175.182:81/
http://18.212.207.84:81/
http://100.27.185.224:81/


SETUP:
create 3 servers
install docker and start the service
hostnamectl set-hostname manager/worker-1/worker-2
Enable 2377 port 

docker swarm init (manager) -- > copy-paste the token to worker nodes
docker node ls

Note: individual containers are not going to replicate.
if we create a service then only containers will be distributed.

SERVICE: it's a way of exposing and managing multiple containers.
in service we can create copy of conatiners.
that container copies will be distributed to all the nodes.

service -- > containers -- > distributed to nodes

docker service create --name movies --replicas 3 -p 81:80 vijaykumar444p/movies:latest
docker service ls		: to list services
docker service inspect movies	: to get complete info of service
docker service ps movies	: to list the containers of movies
docker service scale movies=10	: to scale in the containers
docker service scale movies=3	: to scale out the containers
docker service rollback movies	: to go previous state
docker service logs movies	: to see the logs
docker service rm movies	: to delete the services.

when scale down it follows lifo pattern.
LIFO MEANS LAST-IN FIRST-OUT.

Note: if we delete a container it will recreate automatically itself.
it is called as self healing.


CLUSTER ACTIVIES:
docker swarm leave (worker)	: to make node inactive from cluster
To activate the node copy the token.
docker node rm node-id (manager): to delete worker node which is on down state
docker node inspect node_id	: to get comple info of worker node
docker swarm join-token manager	: to generate the token to join

Note: we cant delete the node which is ready state
if we want to join the node to cluster again we need to paste the token on worker node



DOCKER NETWORKING:
Docker networks are used to make communication between the multiple containers that are running on same or different docker hosts. 

We have different types of docker networks.
Bridge Network		: SAME HOST
Overlay network		: DIFFERENT HOST
Host Network
None network

BRIDGE NETWORK: It is a default network that container will communicate with each other within the same host.

OVERLAY NETWORK: Used to communicate containers with each other across the multiple docker hosts.

HOST NETWORK: When you Want your container IP and ec2 instance IP same then you use host network

NONE NETWORK: When you dont Want The container to get exposed to the world, we use none network. It will not provide any network to our container.


To create a network: docker network create network_name
To see the list: docker network ls
To delete a network: docker network rm network_name
To inspect: docker network inspect network_name
To connect a container to the network: docker network connect network_name container_id/name
docker exec -it cont1 container_name /bin/bash
apt update
apt install iputils-ping -y : command to install ping checks
ping ip-address of cont2

To disconnect from the container: docker network disconnect network_name container_name
To prune: docker network prune



=====================================================================================


K8S:

LIMITATIONS OF DOCKER SWARM:
1. CANT DO AUTO-SCALING AUTOMATICALLY
2. CANT DO LOAD BALANCING AUTOMATICALLY
3. CANT HAVE DEFAULT DASHBOARD
4. WE CANT PLACE CONATINER ON REQUITED SERVER.
5. USED FOR EASY APPS. 

HISTORY:
Initially Google created an internal system called Borg (later called as omega) to manage its thousands of applications later they donated the borg system to cncf and they make it as open source. 
initial name is Borg but later cncf rename it to Kubernetes 
the word kubernetes originated from Greek word called pilot or Hailsmen.
Borg: 2014
K8s first version came in 2015.


INTRO:

IT is an open-source container orchestration platform.
It is used to automates many of the manual processes like deploying, managing, and scaling containerized applications.
Kubernetes was developed by GOOGLE using GO Language.
MEM -- > GOOGLE -- > CLUSTER -- > MULTIPLE APPS OF GOOGLE -- > BORG -- > 
Google donated Borg to CNCF in 2014.
1st version was released in 2015.


ARCHITECTURE:

DOCKER : CNCA
K8S: CNPCA

C : CLUSTER
N : NODE
P : POD
C : CONTAINER
A : APPLICATION


COMPONENTS:
MASTER:

1. API SERVER: communicate with user (takes command execute & give op)
2. ETCD: database of cluster (stores complete info of a cluster ON KEY-VALUE pair)
3. SCHEDULER: select the worker node to shedule pods (depends on hw of node)
4. CONTROLLER: control the k8s objects (n/w, service, Node)

WORKER:

1. KUBELET : its an agent (it will inform all activites to master)
2. KUBEPROXY: it deals with nlw (ip, networks, ports)
3. POD: group of conatiners (inside pod we have app)

Note: all components of a cluster will be created as a pod.


CLUSTER TYPES:

1. SELF MANAGED: WE NEED TO CREATE & MANAGE THEM

minikube = single node cluster
kubeadm = multi node cluster (manual)
kops = multi-node cluster (automation)

2. CLOUD BASED: CLOUD PROVIDERS WILL MANAGE THEM

AWS = EKS = ELASTIC KUBERNETES SERVICE
AZURE = AKS = AZURE KUBERENETS SERVICE
GOOGLE = GKS = GOOGLE KUBERENETS SERVICE



MINIKUBE:
It is a tool used to setup single node cluster on K8's. 
It contains API Servers, ETDC database and container runtime
It is used for development, testing, and experimentation purposes on local. 
Here Master and worker runs on same machine
It is a platform Independent.
Installing Minikube is simple compared to other tools.

NOTE: But we don't implement this in real-time Prod

REQUIREMENTS:

2 CPUs or more
2GB of free memory
20GB of free disk space
Internet connection
Container or virtual machine manager, such as: Docker.

Kubectl is the command line tool for k8s
if we want to execute commands we need to use kubectl.

SETUP:
sudo apt update -y
sudo apt upgrade -y
sudo apt install curl wget apt-transport-https -y
sudo curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo mv minikube-linux-amd64 /usr/local/bin/minikube
sudo chmod +x /usr/local/bin/minikube
sudo minikube version
sudo curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
sudo echo "$(cat kubectl.sha256) kubectl" | sha256sum --check
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
sudo minikube start --driver=docker --force

NOTE: When you download a command as binary file it need to be on /usr/local/bin 
because all the commands in linux will be on /usr/local/bin 
and need to give executable permission for that binary file to work as a  command.



POD:
It is a smallest unit of deployment in K8's.
It is a group of containers.
Pods are ephemeral (short living objects)
Mostly we can use single container inside a pod but if we required, we can create multiple containers inside a same pod.
when we create a pod, containers inside pods can share the same network namespace, and can share the same storage volumes .
While creating pod, we must specify the image, along with any necessary configuration and resource limits.
K8's cannot communicate with containers, they can communicate with only pods.
 We can create this pod in two ways, 
1. Imperative(command) 
2. Declarative (Manifest file)


IMPERATIVE:

kubectl run pod1 --image vinodvanama/paytmmovies:latest
kubectl get pods/pod/po
kubectl get pod -o wide
kubectl describe pod pod1
kubectl delete pod pod1

DECRALATIVE: by uisng file called manifest file

MANDATORY FEILDS: without these feilds we cant create manifest

apiVersion:
kind:
metadata:
spec:


vim pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - image: vinodvanama/paytmtrain:latest
      name: cont1

execution: 
kubectl create -f pod.yml
kubectl get pods/pod/po
kubectl get pod -o wide
kubectl describe pod pod1
kubectl delete -f raham.yml

DRAWBACK: once pod is deleted we can't retrive the pod.

HISTORY:
    1  vim minikube.sh
    2  sh minikube.sh
    3  kubectl get pod
    4  kubectl run pod1 --image nginx
    5  kubectl get pod
    6  kubectl get pods
    7  kubectl get po
    8  kubectl get po -o wide
    9  kubectl describe po pod1
   10  kubectl delete po pod1
   11  kubectl run pod1 --image rahamshaik/paytmmovies:latest
   12  kubectl get po
   13  kubectl delete pod pod1
   14  kubectl run pod1 --image nginx
   15  kubectl get po
   16  kubectl get po -o wide
   17  kubectl describe pod pod1
   18  kubectl delete pod pod1
   19  vim abc.yml
   20  kubectl create -f abc.yml
   21  kubectl get po
   22  kubectl get po -o wide
   23  kubectl describe po pod1
   24  kubectl delete pod pod1
   25  ll
   26  kubectl create -f abc.yml
   27  kubectl delete pod pod1
   28  kubectl get po
   29  kubectl get po -A
   30  kubectl get events
   31  kubectl get events | grep -i warnings
   32  kubectl get events | grep -i warning
   33  history

===============================================================

REPLICASET:
rs -- > pods
it will create multiple copies of same pod.
if we delete one pod automatically it will create new pod.
All the pods will have same config.
only pod names will be differnet.


LABLES: individual pods are difficult to manage because they have different names.
so we give a comman label to group them and work with them together
SELECTOR: Used to select pods with same labels.


use kubectl api-resources for checking the objects info

vim replicaset.yml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest




To list rs		:kubectl get rs/replicaset
To show addtional info	:kubectl get rs -o wide
To show complete info	:kubectl describe rs name-of-rs
To delete the rs	:kubectl delete rs name-of-rs
to get lables of pods 	: kubectl get pods -l app=paytm
TO scale rs		: kubectl scale rs/movies --replicas=10 (LIFO)


LIFO: LAST IN FIRST OUT.
IF A POD IS CREATED LASTLY IT WILL DELETE FIRST WHEN SCALE OUT.

ADV:
Self healing
scaling

DRAWBACKS:
1. we cant rollin and rollout, we cant update the application in rs.

DEPLOYMENT:
deploy -- > rs -- > pods
we can update the application.
its high level k8s objects.

vim deploy.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest


To list deployment	:kubectl get deploy
To show addtional info	:kubectl get deploy -o wide
To show complete info	:kubectl describe deploy name-of-deployment
To delete the deploy	:kubectl delete deploy name-of-deploy
to get lables of pods 	:kubectl get pods -l app=paytm
TO scale deploy		:kubectl scale deploy/name-of-deploy --replicas=10 (LIFO)
To edit deploy		:kubectl edit deploy/name-of-deploy
to show all pod labels	:kubectl get pods --show-labels
To delete all pods	:kubectl delete pod --all

kubectl rollout history deploy/movies
kubectl rollout undo deploy/movies
kubectl rollout status deploy/movies
kubectl rollout pause deploy/movies
kubectl rollout resume deploy/movies


=====================================================================

KOPS:
INFRASTRUCTURE: Resources used to run our application on cloud.
EX: Ec2, VPC, ALB, AGS-------------


Minikube -- > single node cluster
All the pods on single node 
if that node got deleted then all pods will be gone.

KOPS:
kOps, also known as Kubernetes operations.
it is an open-source tool that helps you create, destroy, upgrade, and maintain a highly available, production-grade Kubernetes cluster. 
Depending on the requirement, kOps can also provide cloud infrastructure.
kOps is mostly used in deploying AWS and GCE Kubernetes clusters. 
But officially, the tool only supports AWS. Support for other cloud providers (such as DigitalOcean, GCP, and OpenStack) are in the beta stage.


ADVANTAGES:
	Automates the provisioning of AWS and GCE Kubernetes clusters
	Deploys highly available Kubernetes masters
	Supports rolling cluster updates
	Autocompletion of commands in the command line
	Generates Terraform and CloudFormation configurations
	Manages cluster add-ons.
	Supports state-sync model for dry-runs and automatic idempotency
	Creates instance groups to support heterogeneous clusters

ALTERNATIVES:
Amazon EKS , MINIKUBE, KUBEADM, RANCHER, TERRAFORM.


STEP-1: GIVING PERMISSIONS 

KOps Is a third party tool if it want to create infrastructure on aws 
aws need to give permission for it so we can use IAM user to allocate permission for the kops tool

IAM -- > USER -- > CREATE USER -- > NAME: KOPS -- > Attach Polocies Directly -- > AdministratorAccess -- > NEXT -- > CREATE USER
USER -- > SECURTITY CREDENTIALS -- > CREATE ACCESS KEYS -- > CLI -- > CHECKBOX -- >  CREATE ACCESS KEYS -- > DOWNLOAD 

aws configure (run this command on server)

SETP-2: INSTALL KUBECTL AND KOPS

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
wget https://github.com/kubernetes/kops/releases/download/v1.25.0/kops-linux-amd64
chmod +x kops-linux-amd64 kubectl
mv kubectl /usr/local/bin/kubectl
mv kops-linux-amd64 /usr/local/bin/kops

vim .bashrc
export PATH=$PATH:/usr/local/bin/  -- > save and exit
source .bashrc

SETP-3: CREATING BUCKET 
aws s3api create-bucket --bucket devopsbatchapr600pm.k8s.local --region us-east-1
aws s3api put-bucket-versioning --bucket devopsbatchapr600pm.k8s.local --region us-east-1 --versioning-configuration Status=Enabled
export KOPS_STATE_STORE=s3://devopsbatchapr600pm.k8s.local

SETP-4: CREATING THE CLUSTER
kops create cluster --name rahams.k8s.local --zones us-east-1a --master-count=1 --master-size t2.medium --node-count=2 --node-size t2.micro
kops update cluster --name rahams.k8s.local --yes --admin


Suggestions:
 * list clusters with: kops get cluster
 * edit this cluster with: kops edit cluster rahams.k8s.local
 * edit your node instance group: kops edit ig --name=rahams.k8s.local nodes-us-east-1a
 * edit your master instance group: kops edit ig --name=rahams.k8s.local master-us-east-1a


ADMIN ACTIVITIES:
To scale the worker nodes:
kops edit ig --name=rahams.k8s.local nodes-us-east-1a
kops update cluster --name rahams.k8s.local --yes --admin 
kops rolling-update cluster --yes

ADMIN ACTIVITIES:
kops update cluster --name rahams.k8s.local --yes
kops rolling-update cluster

NOTE: In real time we use fine node cluster to master nodes and three worker nodes.

NOTE: its My humble request for all of you not to delete the cluster manually and do not delete any server use the below command to delete the cluster.

TO DELETE: kops delete cluster --name rahams.k8s.local --yes


NAMESPACES:

NAMESPACE: It is used to divide the cluster to multiple teams on real time.
it is used to isolate the env.

CLUSTER: HOUSE
NAMESPACES: ROOM

Each namespace is isolated.
if your are room-1 are you able to see room-2.
If dev team create a pod on dev ns testing team cant able to access it.
we cant access the objects from one namespace to another namespace.


TYPES:

default           : Is the default namespace, all objects will create here only
kube-node-lease   : it will store object which is taken from one namespace to another.
kube-public	  : all the public objects will store here.      
kube-system 	  : default k8s will create some objects, those are storing on this ns.

NOTE: Every component of Kubernetes cluster is going to create in the form of pod
And all these pods are going to store on kUBE-SYSTEM ns.

kubectl get pod -n kube-system	: to list all pods in kube-system namespace
kubectl get pod -n default	: to list all pods in default namespace
kubectl get pod -n kube-public	: to list all pods in kube-public namespace
kubectl get po -A		: to list all pods in all namespaces
kubectl get po --all-namespaces

kubectl create ns dev	: to create namespace
kubectl config set-context --current --namespace=dev : to switch to the namespace
kubectl config view --minify | grep namespace : to see current namespace
kubectl run dev1 --image nginx
kubectl run dev2 --image nginx
kubectl run dev3 --image nginx
kubectl create ns test	: to create namespace
kubectl config set-context --current --namespace=test : to switch to the namespace
kubectl config view --minify | grep namespace : to see current namespace
kubectl get po -n dev
kubectl delete pod dev1 -n dev
kubectl delete ns dev	: to delete namespace
kubectl delete pod --all: to delete all pods



DAEMONSET: used to create one pod on each workernode.
Its the old version of Deployment.
if we create a new node a pod will be automatically created.
if we delete a old node a pod will be automatically removed.
daemonsets will not be removed at any case in real time.
Usecases: we can create pods for Logging, Monitoring of nodes 


apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/moviespaytm:latest
        ports:
          - containerPort: 80


=====================================================================================

SERVICE: It is used to expose the application in k8s.

TYPES:
1. CLUSTERIP: It will work inside the cluster.
it will not expose to outer world.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: movies
  name: movies-deploy
spec:
  replicas: 10
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
      - name: cont1
        image: rahamshaik/moviespaytm:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: service1
spec:
  type: ClusterIP
  selector:
    app: movies
  ports:
    - port: 80

DRAWBACK:
We cannot use app outside.

2. NODEPORT: It will expose our application in a particular port.
Range: 30000 - 32767 (in sg we need to give all traffic)
if we dont sepcify k8s service will take random port number.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: movies
  name: movies-deploy
spec:
  replicas: 10
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
      - name: cont1
        image: rahamshaik/moviespaytm:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: service1
spec:
  type: NodePort
  selector:
    app: movies
  ports:
    - port: 80
      nodePort: 31111

NOTE: UPDATE THE SG (REMOVE OLD TRAFFIC AND GIVE ALL TRAFFIC)
DRAWBACK:
EXPOSING PUBLIC-IP & PORT 
PORT RESTRICTION.

3. LOADBALACER: It will expose our app and distribute load blw pods.
it will expose the application with dns [Domain Name System] -- > 53
to crete dns we use Route53 

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/trainservice:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: abc
spec:
  type: LoadBalancer
  selector:
    app: swiggy
  ports:
    - port: 80
      targetPort: 80



DAEMONSET: used to create one pod on each workernode.
Its the old version of Deployment.
if we create a new node a pod will be automatically created.
if we delete a old node a pod will be automatically removed.
daemonsets will not be removed at any case in real time.
Usecases: we can create pods for Logging, Monitoring of nodes 


apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/moviespaytm:latest
        ports:
          - containerPort: 80


Replication controller is the previous version of replica set.

RC : uses equity based selector (=, =!)
RS: used set based selector ( env in (dev, test)

apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: rahamshaik/moviespaytm:latest
        ports:
        - containerPort: 80






METRIC SERVER:
if we install metric server in k8s cluster it can collects metrics like cpu, ram -- from all the pods and nodes in cluster.
we can use kubectl top po/no to see metrics
previously we can called it as heapster.

Metrics Server offers:

    A single deployment that works on most clusters (see Requirements)
    Fast autoscaling, collecting metrics every 15 seconds.
    Resource efficiency, using 1 milli core of CPU and 2 MB of memory for each node in a cluster.
    Scalable support up to 5,000 node clusters.


You can use Metrics Server for:
CPU/Memory based horizontal autoscaling (Horizontal Autoscaling)
Automatically adjusting/suggesting resources needed by containers (Vertical Autoscaling)


Horizontal: New 
Vertical: Existing



In Kubernetes, a HorizontalPodAutoscaler automatically updates a workload resource (such as a Deployment or ReplicaSet), with the aim of automatically scaling the workload to match demand.

Example : if you have pod-1 with 50% load and pod2 with 50% load then average will be (50+50/2=50) average value is 50
but if pod-1 is exceeding 60% and pod-2 50% then average will be 55% (then here we need to create a pod-3 becaue its exceeding the average)

Here we need to use metric server whose work is to collect the metrics (cpu & mem info)
metrics server is connected to the HPA and give information to HPA 
Now HPA will analysis metrics for every 30 sec and create a new pod if needed.


COOLING PERIOD:


scaling can be done only for scalable objects (ex: RS, Deployment, RC )
HPA is implemented as a K8S API Resources and a controller.
Controller Periodically adjust the number of replicas in RS, RC and Deployment depends on average.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest


kubectl apply -f hpa.yml
kubectl get all
kubectl get deploy 
kubectl autoscale deployment movies --cpu-percent=20 --min=1 --max=10
kubectl get hpa
kubectl desribe hpa movies
kubectl get al1

open second termina and give
kubectl get po --watch

come to first terminal and go inside pod
kubectl exec mydeploy-6bd88977d5-7s6t8 -it -- /bin/bash

apt update -y
apt install stress -y
stress 

check terminal two to see live pods

======================================================================


QUOTAS:
k8s cluster can be divied into namespaces
By default the pod in K8s will run with no limitations of Memory and CPU
But we need to give the limit for the Pod 
It can limit the objects that can be created in a namespace and total amount of resources.
when we create a pod scheduler will the limits of node to deploy pod on it.
here we can set limits to CPU, Memory and Storage
here CPU is measured on cores and memory in bytes.
1 cpu = 1000 millicpus  ( half cpu = 500 millicpus (or) 0.5 cpu)

Here Request means how many we want
Limit means how many we can create maximum

limit can be given to pods as well as nodes
the default limit is 0

if you mention request and  limit then everything is fine
if you dont mention request and mention limit then Request=Limit
if you mention request and not mention limit then Request=!Limit

IMPORTANT:
Ever Pod in namespace must have CPU limts.
The amount of CPU used by all pods inside namespace must not exceed specified limit.

DEFAULT RANGE:
CPU : 
MIN		= REQUEST = 0.5
MAX		= LIMIT = 1

MEMORY :
MIN	= REQUEST = 500M
MAX	= LIMIT = 1G


kubectl create ns dev
kubectl config set-context $(kubectl config current-context) --namespace=dev

vim dev-quota.yml

apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
  namespace: dev
spec:
  hard:
    pods: "5"
    limits.cpu: "1"
    limits.memory: 1Gi

kubectl create -f dev-quota.yml
kubectl get quota


EX-1: Mentioning Limits  = SAFE WAY

apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest
          resources:
            limits:
              cpu: "1"
              memory: 512Mi

kubectl create -f dep.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest
          resources:
            limits:
              cpu: "0.2"
              memory: 100Mi

kubectl create -f dep.yml



EX-2: MENTION LIMITS & REQUESTS = SAFE WAY


apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: "0.2"
              memory: 100Mi

EX-3: MENTION only REQUESTS =  NOT SAFE WAY


apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest
          resources:
            requests:
              cpu: "0.2"
              memory: 100Mi


 =======================================================================

pv:

Persistent means always available.
PVs are independent they can exist even if no pod is using them.
it is created by administrator or dynamically created by a storage class. 
Once a PV is created, it can be bound to a Persistent Volume Claim (PVC), which is a request for storage by a pod.
When a pod requests storage via a PVC, K8S will search for a suitable PV to satisfy the request. 
PV is bound to the PVC and the pod can use the storage. 
If no suitable PV is found, K8S will either dynamically create a new one (if the storage class supports dynamic provisioning) or the PVC will remain unbound.

pvc:

To use Pv we need to claim the volume using PVC.
PVC request a PV with your desired specification (size, access, modes & speed etc) from k8s and onec a suitable PV is found it will bound to PVC
After bounding is done to pod you can mount it as a volume.
once userfinised its work the attached PV can be released the underlying PV can be reclaimed & recycled for future.

RESTRICTIONS:
1. Instances must be on same az as the ebs 
2. EBS supports only a sinlge EC2 instance mounting

pv.yml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  awsElasticBlockStore:
    volumeID: vol-07e5c6c3fe273239f
    fsType: ext4

pvc.yml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi

dep.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: pvdeploy
spec:
  replicas: 1
  selector:
    matchLabels:
     app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: raham
        image: centos
        command: ["/bin/bash", "-c", "sleep 10000"]
        volumeMounts:
        - name: my-pv
          mountPath: "/tmp/persistent"
      volumes:
        - name: my-pv
          persistentVolumeClaim:
            claimName: my-pvc

kubectl exec pvdeploy-86c99cf54d-d8rj4 -it -- /bin/bash
cd /tmp/persistent/
ls
vim raham
exit

now delete the pod and new pod will created then in that pod you will see the same content.

MULTI CONTAINER PODS:

SIDE CAR:
It creates a helper container to main container.
main container will have application and helper container will do help for maiN container.

Adapter Design Pattern:
standardize the output pattern of main container.

Ambassador Design Pattern:
used to connect containers with the outside world

Init Conatiner:
it initialize the first work and exits later.

=============================================================================

PROMETHEUS:

Prometheus is an open-source monitoring system that is especially well-suited for cloud-native environments, like Kubernetes. 
It can monitor the performance of your applications and services.
it will sends an alert you if there are any issues. 
It has a powerful query language that allows you to analyze the data.
It pulls the real-time metrics, compresses and stores  in a time-series database.
Prometheus is a standalone system, but it can also be used in conjunction with other tools like Alertmanager to send alerts based on the data it collects.
it can be integration with tools like PagerDuty, Teams, Slack, Emails to send alerts to the appropriate on-call personnel.
it collects, and it also has a rich set of integrations with other tools and systems.
For example, you can use Prometheus to monitor the health of your Kubernetes cluster, and use its integration with Grafana to visualize the data it collects.

COMPONENTS OF PROMETHEUS:
Prometheus is a monitoring system that consists of the following components:

A main server that scrapes and stores time series data
A query language called PromQL is used to retrieve and analyze the data
A set of exporters that are used to collect metrics from various systems and applications
A set of alerting rules that can trigger notifications based on the data
An alert manager that handles the routing and suppression of alerts

GRAFANA:
Grafana is an open-source data visualization and monitoring platform that allows you to create dashboards to visualize your data and metrics. 
It is a popular choice for visualizing time series data, and it integrates with a wide range of data sources, including Prometheus, Elasticsearch, and InfluxDB.
A user-friendly interface that allows you to create and customize dashboards with panels that display your data in a variety of formats, including graphs, gauges, and tables. 
You can also use Grafana to set up alerts that trigger notifications when certain conditions are met.
Grafana has a rich ecosystem of plugins and integrations that extend its functionality. For example, you can use Grafana to integrate with other tools and services, such as Slack or PagerDuty, to receive alerts and notifications.

CONNECTION:
SETUP BOTH PROMETHEUS & GRAFAN FROM BELOW LINK
https://github.com/RAHAMSHAIK007/all-setups.git

pROMETHERUS: 9090
NODE EXPORTER: 9100
GRAFANA: 3000

CONNECTING PROMETHEUS TO GARAFANA:
connect to grafana dashboard -- > Data source -- > add -- > promethus -- > url of prometheus -- > save & test -- > top of page -- > explore data -- > if you want run some queries -- > top -- > import dashboard -- > 1860 -- > laod --- > prometheus -- > import 

amazon-linux-extras install epel -y
yum install stress -y


CONNECTING TO WORKER NODES:

Craete 2 servers and install node exporter
go to mai server and 

vim /etc/hosts
public-ip node1  worker-1
public-ip node2  worker-2


SYNOPSIS:
PROMETEUS:
its a free & opensource monitoring tool
it collects metrics of nodes
it store metrics on time series database
we use PromQL language 
we can integrate promethus with tools like
pagerduty, slack and email to send notifications
PORT: 9090

GRAFANA:
its a visualization tool used to create dashboard.
Datasource is main component (from where you are getting data)
Prometheus will show data but cant create dashboards
Dashboards: create, Import  
we can integrate Grafana with tools like
pagerduty, slack and email to send notifications
PORT: 3000

username & password: admin & admin

NODE EXPORTER:
collects metrics of worker nodes
in each worker node we need to install node exporter
Port: 9100

1650
10180
11340
14731



HELM:

In K8S Helm is a package manager to install packages
in Redhat: yum & Ubuntu: apt & K8s: helm 

it is used to install applications on clusters.
we can install and deploy applications by using helm
it manages k8s resources packages through charts 
chart is a collection of files organized on a directory structure.
chart is collection of manifest files.
a running instance of a chart with a specific config is called a release.

INSTALLATION OF HELM:
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
helm version


INSTALLATION OF METRIC SERVER:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/high-availability-1.21+.yaml


INSTALL PROMETHEUS:
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts

UPDATE HELM CHART REPOS:
helm repo update
helm repo list

CREATE PROMETHEUS NAMESPACE:
kubectl create namespace prometheus
kubectl get ns

INSTALL PROMETHEUS:
helm install prometheus prometheus-community/prometheus --namespace prometheus --set alertmanager.persistentVolume.storageClass="gp2" --set server.persistentVolume.storageClass="gp2"
kubectl get pods -n prometheus
kubectl get all -n prometheus

CREATE GRAFANA NAMESPACE:
kubectl create namespace grafana

INSTALL GRAFANA:
helm install grafana grafana/grafana --namespace grafana --set persistence.storageClassName="gp2" --set persistence.enabled=true --set adminPassword='RahamDevOps' --set  service.type=LoadBalancer
kubectl get pods -n grafana
kubectl get service -n grafana

Copy the EXTERNAL-IP and paste in browser

Go to Grafana Dashboard  Add the Datasource  Select the Prometheus
add the below url in Connection and save and test
http://prometheus-server.prometheus.svc.cluster.local/


Import Grafana dashboard from Grafana Labs
grafana dashboard  new  Import  6417  load  select prometheus  import



NOW DEPLOY ANY APPLICATION AND SEE THE RESULT IN DASHBOARD.


ADD 315 PORT TO MONITOR THE FOLLOWING TERMS:
Network I/O pressure.
Cluster CPU usage.
Cluster Memory usage.
Cluster filesystem usage.
Pods CPU usage.

ADD 1860 PORT TO MONITOR NODES INDIVIDUALLY 

11454 -- > for pv and pvcs
747 -- > pod metrics
14623 -- > k8s overview db



SETTING ALREATS:

browser -- > my google account -- > security -- > two setp verification -- > App passwords -- > grafana -- > copy password 

sttx chyr zxmu bqfr 

vim /etc/grafana/grafana.ini
line 892

 892 [smtp]
 893 enabled = true
 894 host = smtp.gmail.com:587
 895 user = devopsbyraham@gmail.com
 896 # If the password contains # or ; you have to wrap it with triple quotes. Ex """#pa     ssword;"""
 897 password = xhfa drlb zgwm ogey
 898 ;cert_file =
 899 ;key_file =
 900 skip_verify = true
 901 from_address = devopsbyraham@gmail.com
 902 from_name = grafana

systemctl restart grafana-server.service
systemctl status grafana-server.service

Grafana -- > Aleart rules -- contact point -- > name -- > email -- > test -- > check gmail

aleating rules -- > new -- > set -- > name: Grafana -- > Metric: node_cpu_seconds_total -- > instace -- > localhost:9100 -- > 

Input: A & Function: Last & Mode: Strict 
Input: B & isabove: 0.7 (70%)

Folder: cpu
Evaluation group: cpu
Pending period: 30s 



ENV VARIABLES:

It is a way to pass configuration information to containers running within pods. 
To set Env   vars it include the env or envFrom field in the configuration file.

ENV: DIRECTLY PASSING
ENVFROM: PASSING FROM FILE

ENV:
allows you to set environment variables for a container, specifying a value directly for each variable that you name.

ENVFROM:
allows you to set environment variables for a container by referencing either a ConfigMap or a Secret. 
When you use envFrom, all the key-value pairs in the referenced ConfigMap or Secret are set as environment variables for the container. 
You can also specify a common prefix string

CONFIGMAPS:

It is used to store the data in key-value pair, files, or command-line arguments that can be used by pods, containers and other resources in cluster
But the data should be non confidential data ()
But it does not provider security and encryption.
If we want to provide encryption use secrets in kubernetes.
Limit of config map data in only 1 MB (we cannot store more than that)
But if we want to store a large amount of data in config maps we have to mount a volume or use a seperate database or file service.


USE CASES:
Configure application setting
Configuring a pod or container
Sharing configuration data across multiple resources
We can store the data: By using this config maps, we can store the data like IP address, URL's and DNS etc...

kubectl create deploy swiggydb --image=mariadb
kubectl get pods
kubectl logs swiggydb-5d49dc56-cbbqk

It is crashed why because we havent specified the password for it


kubectl set env deploy swiggydb MYSQL_ROOT_PASSWORD=Raham123 
kubectl get pods
now it will be on running state
kubectl delete deploy swiggydb

PASSING FROM VAR FILE:
kubectl create deploy swiggydb --image=mariadb
kubectl get pods
kubectl logs swiggydb-5d49dc56-cbbqk

vim vars

MYSQL_ROOT_PASSWORD=Raham123
MYSQL_USER=admin

kubectl create cm dbvars --from-env-file=varsfile
kubectl describe cm dbvars

kubectl get cm
kubectl set env deploy swiggydb --from=configmap/dbvars
kubectl get pods

SECRETS: To store sensitive data in an unencrypted format like passwords, ssh-keys etc ---
it uses base64 encoded format
password=raham (now we can encode and ecode the value)

WHY: if i dont want to expose the sensitive info so we use SECRETS
By default k8s will create some Secrets these are useful from me to create communicate inside the cluster
used to communicate with one resoure to another in cluster
These are system created secrets, we need not to delete

TYPES:
Generic: creates secrets from files, dir, literal (direct values)
TLS: Keys and certs
Docker Registry: used to get private images by using the password

kubectl create deploy swiggydb --image=mariadb
kubectl get po 
kubectl create secret generic password --from-literal=ROOT_PASSWORD=raham123 (from cli)
kubectl create secret generic my-secret --from-env-file=vars (from file)

kubectl get secrets
kubectl describe secret password
kubectl set env deploy swiggydb --from=secrets/password
kubectl get po 
kubectl set env deploy newdb --from=secret/password --prefix=MYSQL_

without passing prefix we cant make the pod running status

TO SEE SECRETS:
kubectl get secrets password -o yaml
echo -n "cmFoYW0xMjM" | base64 -d
echo -n "cmFoYW0xMjM" | base64 --decode

==================================================================================


ARGOCD:

INTRO:
	ArgoCD is a declarative continuous delivery tool for Kubernetes.
	ArgoCD is the core component of Argo Project.
	It helps to automate the deployment and management of applications in a K8s cluster. 
	It uses GitOps methodology to manage the application lifecycle and provides a simple and intuitive UI to monitor the application state, rollout changes, and rollbacks.
	With ArgoCD, you can define the desired state of your Kubernetes applications as YAML manifests and version control them in a Git repository. 
	ArgoCD will continuously monitor the Git repository for changes and automatically apply them to the Kubernetes cluster.
	ArgoCD also provides advanced features like application health monitoring, automated drift detection, and support for multiple environments such as production, staging, and development. 
	It is a popular tool among DevOps teams who want to streamline their Kubernetes application deployment process and ensure consistency and reliability in their infrastructure.



CREATE CLUSTER USING KOPS

INSTALL HELM:
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
helm version

INSTALL ARGO CD USING HELM
helm repo add argo-cd https://argoproj.github.io/argo-helm
helm repo update
kubectl create namespace argocd
helm install argocd argo-cd/argo-cd -n argocd
kubectl get all -n argocd



EXPOSE ARGOCD SERVER:
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'
yum install jq -y
export ARGOCD_SERVER='kubectl get svc argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname''
echo $ARGOCD_SERVER
kubectl get svc argocd-server -n argocd -o json | jq --raw-output .status.loadBalancer.ingress[0].hostname
The above command will provide load balancer URL to access ARGO CD


TO GET ARGO CD PASSWORD:
export ARGO_PWD='kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d'
echo $ARGO_PWD
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
The above command to provide password to access argo cd


CREATE APP AND FORCE IT
SETUP MONOTORING

INGRESS IN K8S:
Ingress helps to expose the HTTP and HTTPS routes from outside of the cluster.
Ingress supports 
Path-based  
Host-based routing
Ingress supports Load balancing and SSL termination.
It redirects the incoming requests to the right services based on the Web URL or path in the address.
Ingress provides the encryption feature and helps to balance the load of the applications.

Ingress is used to manage the external traffic to the services within the cluster which provides features like host-based routing, path-based routing, SSL termination, and more. Where a Load balancer is used to manage the traffic but the load balancer does not provide the fine-grained access control like Ingress.

Example:
Suppose you have multiple Kubernetes services running on your cluster and each service serves a different application such as example.com/app1 and example.com/app2. With the help of Ingress, you can achieve this. However, the Load Balancer routes the traffic based on the ports and can't handle the URL-based routing.

To install ingress, firstly we have to install nginx ingress controller:
command: kubectl create -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.1/deploy/static/provider/cloud/deploy.yaml

Once we install ingress controller, we have to deploy 2 applications. 
github url: https://github.com/mustafaprojectsindevops/kubernetes/tree/master/ingress
After executing all the files, use kubectl get ing to get ingress. After 30 seconds it will provide one load balancer dns.

access those applications using dns/nginx and dns/httpd. So the traffic will route into both the applications as per the routing
=========================================================


INFRASTRUCTURE:
resources used to run our application on cloud.
ex: ec2, s3, elb, vpc, Asg --------------


in general we used to deploy infra on manual 

Manual:
1. time consume
2. Manual work
3. committing mistakes

Automate -- > Terraform -- > code -- > hcl (Hashicorp configuration languge)



its a tool used to make infrastructure automation.
its a free and not open source.
its platform independent.
it comes on the year 2014.
who: Mitchel Hashimoto 
owned: Hashicorp -- > recently IBM is maintaining.
terraform is written on the go language.
We can call terraform as IAAC TOOL.

HOW IT WORKS:
terraform uses code to automate the infra.
we use HCL : HashiCorp Configuration Language.

IAAC: Infrastructure as a code.

Code --- > execute --- > Infra 

ADVANTAGES:
1. Reusable 
2. Time saving
3. Automation
4. Avoiding mistakes
5. Dry run

CLOUD ALTERNATIVES:
CFT = AWS
ARM = AZURE
GDE = GOOGLE

TERRAFROM = ALL CLOUDS

SOME OTHER ALTERNATIVES:
PULUMI
ANSIBLE
CHEF
PUPPET
OpenTofu


TERRAFORM VS ANSIBLE:
Terraform will create server
and these servers will be configure by ansible.


Terraform can be used for on-premises infrastructure. 
While Terraform is known for being cloud-agnostic and supporting public clouds such as AWS, Azure, GCP, it can also be used for on-prem infrastructure including VMware vSphere and OpenStack.

INSTALLING TERRAFORM:

sudo yum install -y yum-utils shadow-utils
sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
sudo yum -y install terraform
aws configure
check ll .aws/ for the configuration 

MAIN ITEMS IN FILE:
blocks
lables (name, type of resource)
arguments


Configuration files:
it will have resource configuration.
here we write inputs for our resource 
based on that input terraform will create the real world resources.
extension is .tf 

mkdir terraform
cd terraform

vim main.tf 

provider "aws" {
region = "us-east-1"
}

resource "aws_instance" "one" {
ami = "ami-03eb6185d756497f8"
instance_type = "t2.micro"
}

I : INIT
P : PLAN
A : APPLY
D : DESTROY

TERRAFORM COMMANDS:
terraform init	: initialize the provider plugins on backend
it will store information of plugins in .terraform folder
without plugins we cant create resources.
each provider will have its own plugins.

terraform plan	: to create an execution plan
it will take inputs given by users and plan the resource creation
if we haven't given inputs for few fields it will take default values.

terraform apply : to create resources
as per the given inputs on configuration file it will create the resources in real word.

terrafrom destroy : to delete resources

provider "aws" {
region = "us-east-1"
}

resource "aws_instance" "one" {
count = 5
ami = "ami-03eb6185d756497f8"
instance_type = "t2.micro"
}

terraform apply --auto-approve
terraform destroy --auto-approve


STATE FILE: used to store the resource information which is created by terraform
to track the resource activities
in real time entire resource info is on state file.
we need to keep it safe & Secure
if we lost this file we cant track the infra.
Command:
terraform state list

terraform target: used to destroy the specific resource 
terraform state list
single target: terraform destroy --auto-approve -target="aws_instance.one[3]"
multi targets: terraform destroy --auto-approve -target="aws_instance.one[1]" -target="aws_instance.one[2]"


TERRAFORM VARIABLES:
in real time we keep all the variables in variable.tf to maintain the variables easily.


main.tf

provider "aws" {
region = "us-east-1"
}

resource "aws_instance" "one" {
count = var.instance_count
ami = "ami-0b41f7055516b991a"
instance_type = var.instance_type
}

variable.tf

variable "instance_type" {
description = "*"
type = string
default = "t2.micro"
}

variable "instance_count" {
description = "*"
type = number
default = 5
}

terraform apply --auto-approve
terraform destroy --auto-approve

TERRAFORM FMT: 
used to give alignment and indentation for terraform files.

========================================================

Terraform tfvars:
When we have multiple configurations for terraform to create resource
we use tfvars to store different configurations.
on execution time pass the tfvars to the command it will apply the values of that file.

cat main.tf
provider "aws" {
region = "us-east-1"
}

resource "aws_instance" "one" {
count = var.instance_count
ami = "ami-0e001c9271cf7f3b9"
instance_type = var.instance_type
tags = {
Name = var.instance_name
}
}

cat variable.tf
variable "instance_count" {
}

variable "instance_type" {
}

variable "instance_name" {
}

cat dev.tfvars
instance_count = 1

instance_type = "t2.micro"

instance_name = "dev-server"

cat test.tfvars
instance_count = 2

instance_type = "t2.medium"

instance_name = "test-server"

cat prod.tfvars
instance_count = 3

instance_type = "t2.large"

instance_name = "prod-server"


TERRAFORM CLI: 

cat main.tf
provider "aws" {
}

resource "aws_instance" "one" {
ami = "ami-00b8917ae86a424c9"
instance_type = var.instance_type
tags = {
Name = "raham-server"
}
}

cat variable.tf
variable "instance_type" {
}

METHOD-1:
terraform apply --auto-approve
terraform destroy --auto-approve

METHOD-2:
terraform apply --auto-approve -var="instance_type=t2.micro"
terraform destroy --auto-approve -var="instance_type=t2.micro"

NOTE: If you want to pass single variable from cli you can use -var or if you want to pass multiple variables from cli create terraform .tfvars files and use -var-file.

TERRAFORM ENV VARIABLES:

export TF_VAR_ami=ami-0195204d5dce06d99

cat main.tf
provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  ami = var.ami
  instance_type = "t2.micro"
  tags = {
    Name = "raham"
  }
}

cat variable.tf
variable "ami" {
  default     = ""
}


TERRAFORM OUTPUTS:
Whenever we create a resource by Terraform if you want to print any output of that resource we can use the output block this block will print the specific output as per our requirement.


provider "aws" {
}

resource "aws_instance" "one" {
ami = "ami-00b8917ae86a424c9"
instance_type = "t2.micro"
tags = {
Name = "raham-server"
}
}

output "raham" {
value = [aws_instance.one.public_ip, aws_instance.one.private_ip, aws_instance.one.public_dns]
}

TO GET COMPLTE OUTPUS:

output "raham" {
value = aws_instance.one
}


Note: when we change output block terraform will execute only that block
remianing blocks will not executed because there are no changes in those blocks.


TERRAFORM TAINT & UNTAINT: 
it is used to recreate specific resources in infrastructure.
Why: 
if i have an ec2 -- > crashed
ec2 -- > code -- > main.tf 
now to recreate this ec2 seperately we need to taint the resource


provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  ami           = "ami-0195204d5dce06d99"
  instance_type = "t2.micro"
  tags = {
    Name = "raham"
  }
}

resource "aws_s3_bucket" "two" {
  bucket = "rahamshaik8e3huirfh9uf2f"
}

terraform apply --auto-approve 

terraform state list
terraform taint aws_instance.one
terraform apply --auto-approve 

TO TAINT: terraform taint aws_instance.one
TO UNTAINT: terraform untaint aws_instance.one

TERRAFORM REPLACE:
terraform apply --auto-approve  -replace="aws_instance.one[0]"
=============================================================


TERRAFORM LOCALS: its a block used to define values
once you define a value on this block you can use them multiple times
changing the value in local block will be replicated to all resources.
simply define value once and use for mutiple times.



provider "aws" {
}

locals {
env = "prod"
}

resource "aws_vpc" "one" {
cidr_block = "10.0.0.0/16"
tags = {
Name = "${local.env}-vpc"
}
}

resource "aws_subnet" "two" {
vpc_id = aws_vpc.one.id
cidr_block = "10.0.0.0/24"
tags = {
Name = "${local.env}-subnet"
}
}

resource "aws_instance" "three" {
subnet_id = aws_subnet.two.id
ami = "ami-00b8917ae86a424c9"
instance_type = "t2.micro"
key_name = "jrb"
tags = {
Name = "${local.env}-server"
}
}

Note: values will be updated when we change them on same workspace.


WORKSPACES:
it is used to create infra for multiple env 
it will isolate each env
if we work on dev env it wont affect test env
the default workspace is default 
all the resource we create on terraform by default will store on default workspace
all workspace state files will be stored on terraform.tfstate.d folder


terraform workspace list	: to list the workspaces
terraform workspace new dev	: to create workspace
terraform workspace show	: to show current workspace
terraform workspace select dev	: to switch to dev workspace
terraform workspace delete dev	: to delete dev workspace


NOTE:
1. we need to empty the workspace before delete
2. we cant delete current workspace, we can switch and delete
3. we cant delete default workspace


EXECUTION:

provider "aws" {
region = "us-east-1"
}

locals {
env = "${terraform.workspace}"
}

resource "aws_vpc" "one" {
cidr_block = "10.0.0.0/16"
tags = {
Name = "${local.env}-vpc"
}
}

resource "aws_subnet" "two" {
vpc_id = aws_vpc.one.id
cidr_block = "10.0.0.0/24"
tags = {
Name = "${local.env}-subnet"
}
}

resource "aws_instance" "three" {
subnet_id = aws_subnet.two.id
ami = "ami-0e001c9271cf7f3b9"
instance_type = "t2.micro"
tags = {
Name = "${local.env}-server"
}
}


s3 Backend setup: 
it will store terraform statefile in bucket.
when we modify the infra it will update the statefile in bucket.

why: state file is very imp in terraform
without state file we cant track the infra
if you lost it we cant manage the infra

backup file is a backup of the terraform. tfstate file. Terraform automatically creates a backup of the state file before making any changes to the state file. This ensures that you can recover from a corrupted or lost state file.

terraform state list : to list the resources
terraform state show aws_subnet.two : to show specific resource info
terraform state mv aws_subnet.two aws_subnet.three : to move state info from one to another
terraform state rm aws_subnet.three : to remove state information of a resource
terraform state pull : to pull state file info from backend

terraform init -migrate-state
terraform init -reconfigure : to bring state file to local

CODE:
provider "aws" {
region = "us-east-1"
}

terraform {
  backend "s3" {
    bucket = "terrastatebyucket007"
    key    = "terraform.tfstate"
    region = "us-east-1"
  }
}

locals {
env = "${terraform.workspace}"
}

resource "aws_vpc" "one" {
cidr_block = "10.0.0.0/16"
tags = {
Name = "${local.env}-vpc"
}
}

resource "aws_subnet" "two" {
vpc_id = aws_vpc.one.id
cidr_block = "10.0.0.0/24"
tags = {
Name = "${local.env}-subnet"
}
}

resource "aws_instance" "three" {
subnet_id = aws_subnet.two.id
ami = "ami-0e001c9271cf7f3b9"
instance_type = "t2.micro"
tags = {
Name = "${local.env}-server"
}
}


==============================================================

META ARGUMENTS:

DEPENDS_ON: One resource creation depends on another resource.
used to manage dependencies of resources.


provider "aws" {
}

resource "aws_instance" "three" {
  ami           = "ami-00b8917ae86a424c9"
  instance_type = "t2.medium"
  tags = {
    Name = "n.virginia-server"
  }
  depends_on = [
    aws_s3_bucket.three
  ]
}

resource "aws_s3_bucket" "three" {
  bucket = "rahamshaik00998877abc"
}

COUNT: count is to create identical objects which is having same configuration.

provider "aws" {
}

resource "aws_instance" "three" {
  count         = 3
  ami           = "ami-00b8917ae86a424c9"
  instance_type = "t2.medium"
  tags = {
    Name = "dev-server"   (TO GIVE NUMBERS ADD -- > -${count.index+1})
  }
}


provider "aws" {
}

resource "aws_instance" "three" {
  count         = length(var.instance_type)
  ami           = "ami-00b8917ae86a424c9"
  instance_type = var.instance_type[count.index]
  tags = {
    Name = var.instance_name[count.index]
  }
}

variable "instance_type" {
  default = ["t2.micro", "t2.medium", "t2.large"]
}

variable "instance_name" {
  default = ["dev-server", "test-server", "prod-server"]
}



FOR_EACH:


resource "aws_instance" "two" {
  for_each      = toset(["dev-server", "test-server", "prod-server"])
  ami           = "ami-00b8917ae86a424c9"
  instance_type = "t2.micro"
  tags = {
    Name = "${each.key}"
  }
}



LIFECYCLE:

PREVENT DESTROY: used to prevent the resources from detroying.

provider "aws" {
}

resource "aws_instance" "two" {
  ami           = "ami-0d7a109bf30624c99"
  instance_type = "t2.nano"
  tags = {
    Name = "lucky-server"
  }
  lifecycle {
    prevent_destroy = true
  }
}



CREATE BEFORE DESTROY:  
If we want to recreate any object in terraform. first of all terraform will destroy the existing object and then it will create the new object.

it will create new replacement object is created first, & destroyed the existing resource.

NOTE: Change ami-id and run apply to see the changes
provider "aws" {
}

resource "aws_instance" "two" {
  ami           = "ami-0d7a109bf30624c99"
  instance_type = "t2.nano"
  tags = {
    Name = "lucky-server"
  }
  lifecycle {
    create_before_destroy = true
  }
}

IGNORE CHANGES: Whenever we do any changes to the infrastructure manually if I run terraform plan or if I run terraform apply the values will be taken to the terraform state if I want to ignore the manual changes made to my infrastructure we can use ignore changes.


NOTE: It is mainly used to ignore the manual changes applied to the infrastructure if you apply any change to the existing infrastructure manually terraform will completely ignore during the runtime


provider "aws" {
}

resource "aws_instance" "two" {
  ami           = "ami-0d7a109bf30624c99"
  instance_type = "t2.nano"
  tags = {
    Name = "lucky-server"
  }
  lifecycle {
    ignore_changes = all
  }
}


Providers:
Terraform will support thousands of providers in real time but among them we are not going to use some specific providers which is going to maintain by community.

1. OFFICIAL: Maintain by Terraform
2. Partner: Maintain by Terraform & that org
3. Community: Maintain by Individuals.


GITHUB:

provider "github" {
token = "***********************"
}

resource "github_repository" "example_repo" {
  name        = "example-repo"
  description = "This is an example repository created with Terraform"
}

LOCAL:

provider "local" {
}


resource "local_file" "one" {
  filename = "abc.txt"
  content  = "hai all my file is created by terraform"
}


NOTE: For every provider in Terraform we need to download the plugins by running terraform init.

=====================================================
VERSION CONSTRAINTS:
we can change the versions of provider plugins.
Whenever we have new changes on the aws console the old code might not work so if you want to work with the new code window download the new provider plugins for the new code in real time we update the plugins based upon our requirement.

terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "5.41.0"
    }
  }
}


terraform {
  required_providers {
    local = {
      source = "hashicorp/local"
      version = "2.2.2"
    }
  }
}

terraform import


import {
  to = aws_instance.example
  id = var.instance_id
}

terraform plan-generate-config-out=ec2.tf
terraform apply

TERRAFORM REFRESH:

TERRAFORM REFRESH:
it will store the values when compared with real world infrastructure when we modified the terraform values in real world infrastructure it does not replicate to state file 
so we need to run the command called Terraform Refresh it will refresh the state file while refreshing state file it will compare original values with the state file values if the original values are modified or change it it will be replicated to state file after running terraform refresh command.

terraform refresh
when we run plan, apply or destroy refresh will perform automatically.


Note: change somethinng maually and check it
DISADVATAGE: Sometimes it will delete all of the existing infrastructure due to some small sort of changes so in real time we never run this command manually.


TERRAFORM MODULES:
used for reuseable
it divides the code into folder structure.
A module that has been called by another module is often referred to as a child module.
we can publish modules for others to use, and to use modules that others have published.
These modules are free to use, and Terraform can download them automatically if you specify the appropriate source and version in a module call block.


cat main.tf
provider "aws" {
}

module "my_instance" {
source = "./modules/instances"
}

module "s3_module" {
source = "./modules/buckets"
}

mkdir -p modules/instances
mkdir -p modules/buckets

cat modules/buckets/main.tf
resource "aws_s3_bucket" "abcd" {
bucket = "devopsherahamshaik0099889977"
}


cat modules/instance/main.tf
resource "aws_instance" "three" {
  count         = 2
  ami           = "ami-00b8917ae86a424c9"
  instance_type = "t2.medium"
  key_name      = "yterraform"
  tags = {
    Name = "n.virginia-server"
  }
}
